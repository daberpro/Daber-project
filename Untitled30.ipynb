{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "14lEXdsqJwjBuq1N1_PlK6nWPFXiUtHwV",
      "authorship_tag": "ABX9TyN35xBnKBv/nQVwElXMRHuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daberpro/Daber-project/blob/master/Untitled30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44Bo1qunlGjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90ed253-df9b-40b5-f789-5a24d73cc107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "root_path = \"drive/MyDrive/Dataset/CustomDataset/Dataset/Brain Tumor\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not os.path.exists(root_path):\n",
        "  print(\"Cannot Find root dataset!\")\n",
        "\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforming = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.Resize((300, 300)),\n",
        "  torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "  torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "  torchvision.transforms.RandomRotation(10),\n",
        "  torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "  torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "  torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "training_dataset = torchvision.datasets.ImageFolder(\n",
        "  root=root_path+\"/Training\",\n",
        "  transform=transforming\n",
        ")\n",
        "\n",
        "testing_dataset = torchvision.datasets.ImageFolder(\n",
        "  root=root_path+\"/Testing\",\n",
        "  transform=torchvision.transforms.Compose([\n",
        "      torchvision.transforms.Resize((300, 300)),\n",
        "      torchvision.transforms.ToTensor()\n",
        "  ])\n",
        ")\n",
        "\n",
        "print(training_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoQcpoASGkol",
        "outputId": "6bfb59a8-809d-4c23-81f9-1698aad54d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['glioma', 'meningioma', 'notumor', 'pituitary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(EncoderModel, self).__init__()\n",
        "    self.seq = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(3,16,3,1,1),\n",
        "        torch.nn.BatchNorm2d(16),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.MaxPool2d(2,2),\n",
        "        torch.nn.Conv2d(16,32,3,1,1),\n",
        "        torch.nn.BatchNorm2d(32),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.MaxPool2d(2,2),\n",
        "        torch.nn.Conv2d(32,64,3,1,1),\n",
        "        torch.nn.BatchNorm2d(64),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.MaxPool2d(2,2),\n",
        "        torch.nn.Conv2d(64,128,3,1,1),\n",
        "        torch.nn.BatchNorm2d(128),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.MaxPool2d(2,2),\n",
        "        torch.nn.Conv2d(128,256,3,1,1),\n",
        "        torch.nn.BatchNorm2d(256),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.AdaptiveMaxPool2d((1,1)),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Linear(256,64),\n",
        "        torch.nn.PReLU(),\n",
        "        torch.nn.Linear(64,4)\n",
        "    );\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.seq(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dkY2R3lRHGUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderModel().to(device)\n",
        "if os.path.exists(\"EncoderModel.pth\"):\n",
        "  model.load_state_dict(torch.load(\"EncoderModel.pth\"))\n",
        "epoch = 10\n",
        "optimizer = torch.optim.AdamW(model.parameters(), 0.0001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_data = torch.utils.data.DataLoader(training_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "for ep in range(epoch):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (img, label) in enumerate(train_data):\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(img)\n",
        "        loss = loss_fn(output, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        print(f\"Epoch {ep+1} Batch {batch_idx+1}/{len(train_data)} Loss: {loss.item()}\")\n",
        "        torch.save(model.state_dict(), \"EncoderModel.pth\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_data)\n",
        "    print(f\"Epoch {ep+1}/{epoch} Avg Loss: {avg_loss}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"EncoderModel.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrIyB7ntIoqe",
        "outputId": "bedfc50b-26aa-4609-802e-b83d8e063aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 1/179 Loss: 0.037525393068790436\n",
            "Epoch 1 Batch 2/179 Loss: 0.014306031167507172\n",
            "Epoch 1 Batch 3/179 Loss: 0.00420041847974062\n",
            "Epoch 1 Batch 4/179 Loss: 0.05879458039999008\n",
            "Epoch 1 Batch 5/179 Loss: 0.011387703940272331\n",
            "Epoch 1 Batch 6/179 Loss: 0.12784996628761292\n",
            "Epoch 1 Batch 7/179 Loss: 0.00811939686536789\n",
            "Epoch 1 Batch 8/179 Loss: 0.008016716688871384\n",
            "Epoch 1 Batch 9/179 Loss: 0.0035781830083578825\n",
            "Epoch 1 Batch 10/179 Loss: 0.02178524062037468\n",
            "Epoch 1 Batch 11/179 Loss: 0.017587490379810333\n",
            "Epoch 1 Batch 12/179 Loss: 0.0491177998483181\n",
            "Epoch 1 Batch 13/179 Loss: 0.029473524540662766\n",
            "Epoch 1 Batch 14/179 Loss: 0.09772884845733643\n",
            "Epoch 1 Batch 15/179 Loss: 0.03716103732585907\n",
            "Epoch 1 Batch 16/179 Loss: 0.012060213834047318\n",
            "Epoch 1 Batch 17/179 Loss: 0.006043911911547184\n",
            "Epoch 1 Batch 18/179 Loss: 0.14393028616905212\n",
            "Epoch 1 Batch 19/179 Loss: 0.007423162925988436\n",
            "Epoch 1 Batch 20/179 Loss: 0.010404416359961033\n",
            "Epoch 1 Batch 21/179 Loss: 0.03658711537718773\n",
            "Epoch 1 Batch 22/179 Loss: 0.006040231324732304\n",
            "Epoch 1 Batch 23/179 Loss: 0.06153644993901253\n",
            "Epoch 1 Batch 24/179 Loss: 0.03322230651974678\n",
            "Epoch 1 Batch 25/179 Loss: 0.05192461609840393\n",
            "Epoch 1 Batch 26/179 Loss: 0.06459778547286987\n",
            "Epoch 1 Batch 27/179 Loss: 0.05836683511734009\n",
            "Epoch 1 Batch 28/179 Loss: 0.01682736538350582\n",
            "Epoch 1 Batch 29/179 Loss: 0.01819569058716297\n",
            "Epoch 1 Batch 30/179 Loss: 0.040036894381046295\n",
            "Epoch 1 Batch 31/179 Loss: 0.011047998443245888\n",
            "Epoch 1 Batch 32/179 Loss: 0.016637047752738\n",
            "Epoch 1 Batch 33/179 Loss: 0.007856783457100391\n",
            "Epoch 1 Batch 34/179 Loss: 0.009101975709199905\n",
            "Epoch 1 Batch 35/179 Loss: 0.08855339884757996\n",
            "Epoch 1 Batch 36/179 Loss: 0.053244732320308685\n",
            "Epoch 1 Batch 37/179 Loss: 0.06961971521377563\n",
            "Epoch 1 Batch 38/179 Loss: 0.09957774728536606\n",
            "Epoch 1 Batch 39/179 Loss: 0.011780805885791779\n",
            "Epoch 1 Batch 40/179 Loss: 0.017758909612894058\n",
            "Epoch 1 Batch 41/179 Loss: 0.013584346510469913\n",
            "Epoch 1 Batch 42/179 Loss: 0.06392502039670944\n",
            "Epoch 1 Batch 43/179 Loss: 0.005979414097964764\n",
            "Epoch 1 Batch 44/179 Loss: 0.020070241764187813\n",
            "Epoch 1 Batch 45/179 Loss: 0.04442403465509415\n",
            "Epoch 1 Batch 46/179 Loss: 0.012386158108711243\n",
            "Epoch 1 Batch 47/179 Loss: 0.007780060172080994\n",
            "Epoch 1 Batch 48/179 Loss: 0.06298604607582092\n",
            "Epoch 1 Batch 49/179 Loss: 0.005040974821895361\n",
            "Epoch 1 Batch 50/179 Loss: 0.01889154687523842\n",
            "Epoch 1 Batch 51/179 Loss: 0.009123148396611214\n",
            "Epoch 1 Batch 52/179 Loss: 0.018587524071335793\n",
            "Epoch 1 Batch 53/179 Loss: 0.05188596248626709\n",
            "Epoch 1 Batch 54/179 Loss: 0.029840368777513504\n",
            "Epoch 1 Batch 55/179 Loss: 0.06778820604085922\n",
            "Epoch 1 Batch 56/179 Loss: 0.10256778448820114\n",
            "Epoch 1 Batch 57/179 Loss: 0.016555335372686386\n",
            "Epoch 1 Batch 58/179 Loss: 0.018961133435368538\n",
            "Epoch 1 Batch 59/179 Loss: 0.012061653658747673\n",
            "Epoch 1 Batch 60/179 Loss: 0.058463890105485916\n",
            "Epoch 1 Batch 61/179 Loss: 0.00582759827375412\n",
            "Epoch 1 Batch 62/179 Loss: 0.019405249506235123\n",
            "Epoch 1 Batch 63/179 Loss: 0.0682012289762497\n",
            "Epoch 1 Batch 64/179 Loss: 0.04730801284313202\n",
            "Epoch 1 Batch 65/179 Loss: 0.09901338815689087\n",
            "Epoch 1 Batch 66/179 Loss: 0.07010002434253693\n",
            "Epoch 1 Batch 67/179 Loss: 0.006235355045646429\n",
            "Epoch 1 Batch 68/179 Loss: 0.01247053686529398\n",
            "Epoch 1 Batch 69/179 Loss: 0.04129805415868759\n",
            "Epoch 1 Batch 70/179 Loss: 0.002301870845258236\n",
            "Epoch 1 Batch 71/179 Loss: 0.07377404719591141\n",
            "Epoch 1 Batch 72/179 Loss: 0.006939588580280542\n",
            "Epoch 1 Batch 73/179 Loss: 0.018452944234013557\n",
            "Epoch 1 Batch 74/179 Loss: 0.023716773837804794\n",
            "Epoch 1 Batch 75/179 Loss: 0.02425859309732914\n",
            "Epoch 1 Batch 76/179 Loss: 0.044719941914081573\n",
            "Epoch 1 Batch 77/179 Loss: 0.028624949976801872\n",
            "Epoch 1 Batch 78/179 Loss: 0.057916060090065\n",
            "Epoch 1 Batch 79/179 Loss: 0.07945095002651215\n",
            "Epoch 1 Batch 80/179 Loss: 0.0032242201268672943\n",
            "Epoch 1 Batch 81/179 Loss: 0.09741313755512238\n",
            "Epoch 1 Batch 82/179 Loss: 0.013771509751677513\n",
            "Epoch 1 Batch 83/179 Loss: 0.006267089396715164\n",
            "Epoch 1 Batch 84/179 Loss: 0.013613248243927956\n",
            "Epoch 1 Batch 85/179 Loss: 0.07166516780853271\n",
            "Epoch 1 Batch 86/179 Loss: 0.030079316347837448\n",
            "Epoch 1 Batch 87/179 Loss: 0.07804033905267715\n",
            "Epoch 1 Batch 88/179 Loss: 0.07066978514194489\n",
            "Epoch 1 Batch 89/179 Loss: 0.025530997663736343\n",
            "Epoch 1 Batch 90/179 Loss: 0.03476863354444504\n",
            "Epoch 1 Batch 91/179 Loss: 0.018820704892277718\n",
            "Epoch 1 Batch 92/179 Loss: 0.011014660820364952\n",
            "Epoch 1 Batch 93/179 Loss: 0.024959590286016464\n",
            "Epoch 1 Batch 94/179 Loss: 0.02807685174047947\n",
            "Epoch 1 Batch 95/179 Loss: 0.011481638997793198\n",
            "Epoch 1 Batch 96/179 Loss: 0.00564007181674242\n",
            "Epoch 1 Batch 97/179 Loss: 0.12290959060192108\n",
            "Epoch 1 Batch 98/179 Loss: 0.022731132805347443\n",
            "Epoch 1 Batch 99/179 Loss: 0.06069580465555191\n",
            "Epoch 1 Batch 100/179 Loss: 0.13242770731449127\n",
            "Epoch 1 Batch 101/179 Loss: 0.009039944969117641\n",
            "Epoch 1 Batch 102/179 Loss: 0.0025057822931557894\n",
            "Epoch 1 Batch 103/179 Loss: 0.007202459499239922\n",
            "Epoch 1 Batch 104/179 Loss: 0.019764995202422142\n",
            "Epoch 1 Batch 105/179 Loss: 0.03223050385713577\n",
            "Epoch 1 Batch 106/179 Loss: 0.04255039617419243\n",
            "Epoch 1 Batch 107/179 Loss: 0.030032522976398468\n",
            "Epoch 1 Batch 108/179 Loss: 0.022969434037804604\n",
            "Epoch 1 Batch 109/179 Loss: 0.03790467977523804\n",
            "Epoch 1 Batch 110/179 Loss: 0.04152939096093178\n",
            "Epoch 1 Batch 111/179 Loss: 0.05148347094655037\n",
            "Epoch 1 Batch 112/179 Loss: 0.03326164186000824\n",
            "Epoch 1 Batch 113/179 Loss: 0.037934862077236176\n",
            "Epoch 1 Batch 114/179 Loss: 0.039744384586811066\n",
            "Epoch 1 Batch 115/179 Loss: 0.0720803290605545\n",
            "Epoch 1 Batch 116/179 Loss: 0.04986412078142166\n",
            "Epoch 1 Batch 117/179 Loss: 0.017607996240258217\n",
            "Epoch 1 Batch 118/179 Loss: 0.06361106038093567\n",
            "Epoch 1 Batch 119/179 Loss: 0.024813279509544373\n",
            "Epoch 1 Batch 120/179 Loss: 0.03384130075573921\n",
            "Epoch 1 Batch 121/179 Loss: 0.07686775177717209\n",
            "Epoch 1 Batch 122/179 Loss: 0.07156561315059662\n",
            "Epoch 1 Batch 123/179 Loss: 0.011456888169050217\n",
            "Epoch 1 Batch 124/179 Loss: 0.014292063191533089\n",
            "Epoch 1 Batch 125/179 Loss: 0.046095579862594604\n",
            "Epoch 1 Batch 126/179 Loss: 0.018132073804736137\n",
            "Epoch 1 Batch 127/179 Loss: 0.018183132633566856\n",
            "Epoch 1 Batch 128/179 Loss: 0.0050188517197966576\n",
            "Epoch 1 Batch 129/179 Loss: 0.006903951987624168\n",
            "Epoch 1 Batch 130/179 Loss: 0.07377294450998306\n",
            "Epoch 1 Batch 131/179 Loss: 0.01585765741765499\n",
            "Epoch 1 Batch 132/179 Loss: 0.03890400379896164\n",
            "Epoch 1 Batch 133/179 Loss: 0.0016715512610971928\n",
            "Epoch 1 Batch 134/179 Loss: 0.015356000512838364\n",
            "Epoch 1 Batch 135/179 Loss: 0.01957479678094387\n",
            "Epoch 1 Batch 136/179 Loss: 0.03636075556278229\n",
            "Epoch 1 Batch 137/179 Loss: 0.017600802704691887\n",
            "Epoch 1 Batch 138/179 Loss: 0.010927942581474781\n",
            "Epoch 1 Batch 139/179 Loss: 0.024223631247878075\n",
            "Epoch 1 Batch 140/179 Loss: 0.04803326725959778\n",
            "Epoch 1 Batch 141/179 Loss: 0.010741472244262695\n",
            "Epoch 1 Batch 142/179 Loss: 0.035198479890823364\n",
            "Epoch 1 Batch 143/179 Loss: 0.03899030759930611\n",
            "Epoch 1 Batch 144/179 Loss: 0.004069192335009575\n",
            "Epoch 1 Batch 145/179 Loss: 0.02339228056371212\n",
            "Epoch 1 Batch 146/179 Loss: 0.005676122847944498\n",
            "Epoch 1 Batch 147/179 Loss: 0.06024793162941933\n",
            "Epoch 1 Batch 148/179 Loss: 0.07633810490369797\n",
            "Epoch 1 Batch 149/179 Loss: 0.010336030274629593\n",
            "Epoch 1 Batch 150/179 Loss: 0.003676183056086302\n",
            "Epoch 1 Batch 151/179 Loss: 0.03360038995742798\n",
            "Epoch 1 Batch 152/179 Loss: 0.14047369360923767\n",
            "Epoch 1 Batch 153/179 Loss: 0.023878825828433037\n",
            "Epoch 1 Batch 154/179 Loss: 0.10823823511600494\n",
            "Epoch 1 Batch 155/179 Loss: 0.023181535303592682\n",
            "Epoch 1 Batch 156/179 Loss: 0.08146537840366364\n",
            "Epoch 1 Batch 157/179 Loss: 0.024268705397844315\n",
            "Epoch 1 Batch 158/179 Loss: 0.04221952334046364\n",
            "Epoch 1 Batch 159/179 Loss: 0.05210394039750099\n",
            "Epoch 1 Batch 160/179 Loss: 0.02726736105978489\n",
            "Epoch 1 Batch 161/179 Loss: 0.02947133779525757\n",
            "Epoch 1 Batch 162/179 Loss: 0.16410936415195465\n",
            "Epoch 1 Batch 163/179 Loss: 0.043898701667785645\n",
            "Epoch 1 Batch 164/179 Loss: 0.012013672851026058\n",
            "Epoch 1 Batch 165/179 Loss: 0.011778810992836952\n",
            "Epoch 1 Batch 166/179 Loss: 0.020470302551984787\n",
            "Epoch 1 Batch 167/179 Loss: 0.02418072335422039\n",
            "Epoch 1 Batch 168/179 Loss: 0.02967638522386551\n",
            "Epoch 1 Batch 169/179 Loss: 0.1042698323726654\n",
            "Epoch 1 Batch 170/179 Loss: 0.015494856052100658\n",
            "Epoch 1 Batch 171/179 Loss: 0.05045469105243683\n",
            "Epoch 1 Batch 172/179 Loss: 0.05359633266925812\n",
            "Epoch 1 Batch 173/179 Loss: 0.07531451433897018\n",
            "Epoch 1 Batch 174/179 Loss: 0.059558093547821045\n",
            "Epoch 1 Batch 175/179 Loss: 0.003689736593514681\n",
            "Epoch 1 Batch 176/179 Loss: 0.014629039913415909\n",
            "Epoch 1 Batch 177/179 Loss: 0.0421382337808609\n",
            "Epoch 1 Batch 178/179 Loss: 0.042785536497831345\n",
            "Epoch 1 Batch 179/179 Loss: 0.06529068946838379\n",
            "Epoch 1/10 Avg Loss: 0.037221700046731775\n",
            "Epoch 2 Batch 1/179 Loss: 0.021553196012973785\n",
            "Epoch 2 Batch 2/179 Loss: 0.04721013456583023\n",
            "Epoch 2 Batch 3/179 Loss: 0.002231224672868848\n",
            "Epoch 2 Batch 4/179 Loss: 0.03916086256504059\n",
            "Epoch 2 Batch 5/179 Loss: 0.006060151848942041\n",
            "Epoch 2 Batch 6/179 Loss: 0.007842326536774635\n",
            "Epoch 2 Batch 7/179 Loss: 0.006092430092394352\n",
            "Epoch 2 Batch 8/179 Loss: 0.021051939576864243\n",
            "Epoch 2 Batch 9/179 Loss: 0.011125346645712852\n",
            "Epoch 2 Batch 10/179 Loss: 0.016855256631970406\n",
            "Epoch 2 Batch 11/179 Loss: 0.01650187000632286\n",
            "Epoch 2 Batch 12/179 Loss: 0.021102599799633026\n",
            "Epoch 2 Batch 13/179 Loss: 0.027640027925372124\n",
            "Epoch 2 Batch 14/179 Loss: 0.038867585361003876\n",
            "Epoch 2 Batch 15/179 Loss: 0.022582707926630974\n",
            "Epoch 2 Batch 16/179 Loss: 0.0747460424900055\n",
            "Epoch 2 Batch 17/179 Loss: 0.021420009434223175\n",
            "Epoch 2 Batch 18/179 Loss: 0.06782501935958862\n",
            "Epoch 2 Batch 19/179 Loss: 0.021631963551044464\n",
            "Epoch 2 Batch 20/179 Loss: 0.057041268795728683\n",
            "Epoch 2 Batch 21/179 Loss: 0.009065423160791397\n",
            "Epoch 2 Batch 22/179 Loss: 0.048304617404937744\n",
            "Epoch 2 Batch 23/179 Loss: 0.002464074408635497\n",
            "Epoch 2 Batch 24/179 Loss: 0.026670964434742928\n",
            "Epoch 2 Batch 25/179 Loss: 0.0511736124753952\n",
            "Epoch 2 Batch 26/179 Loss: 0.0010929543059319258\n",
            "Epoch 2 Batch 27/179 Loss: 0.025937803089618683\n",
            "Epoch 2 Batch 28/179 Loss: 0.01437754649668932\n",
            "Epoch 2 Batch 29/179 Loss: 0.046660199761390686\n",
            "Epoch 2 Batch 30/179 Loss: 0.022593719884753227\n",
            "Epoch 2 Batch 31/179 Loss: 0.012982401996850967\n",
            "Epoch 2 Batch 32/179 Loss: 0.08794908225536346\n",
            "Epoch 2 Batch 33/179 Loss: 0.01067713275551796\n",
            "Epoch 2 Batch 34/179 Loss: 0.005588078871369362\n",
            "Epoch 2 Batch 35/179 Loss: 0.01667783036828041\n",
            "Epoch 2 Batch 36/179 Loss: 0.010397830978035927\n",
            "Epoch 2 Batch 37/179 Loss: 0.06403166055679321\n",
            "Epoch 2 Batch 38/179 Loss: 0.014583825133740902\n",
            "Epoch 2 Batch 39/179 Loss: 0.02391214109957218\n",
            "Epoch 2 Batch 40/179 Loss: 0.009170549921691418\n",
            "Epoch 2 Batch 41/179 Loss: 0.02929890714585781\n",
            "Epoch 2 Batch 42/179 Loss: 0.03621629625558853\n",
            "Epoch 2 Batch 43/179 Loss: 0.009199390187859535\n",
            "Epoch 2 Batch 44/179 Loss: 0.04200233146548271\n",
            "Epoch 2 Batch 45/179 Loss: 0.09306178241968155\n",
            "Epoch 2 Batch 46/179 Loss: 0.0200742706656456\n",
            "Epoch 2 Batch 47/179 Loss: 0.0017041780520230532\n",
            "Epoch 2 Batch 48/179 Loss: 0.08913765847682953\n",
            "Epoch 2 Batch 49/179 Loss: 0.20890255272388458\n",
            "Epoch 2 Batch 50/179 Loss: 0.03319532796740532\n",
            "Epoch 2 Batch 51/179 Loss: 0.07219506800174713\n",
            "Epoch 2 Batch 52/179 Loss: 0.01730438880622387\n",
            "Epoch 2 Batch 53/179 Loss: 0.014517920091748238\n",
            "Epoch 2 Batch 54/179 Loss: 0.015549537725746632\n",
            "Epoch 2 Batch 55/179 Loss: 0.012397035956382751\n",
            "Epoch 2 Batch 56/179 Loss: 0.03742310032248497\n",
            "Epoch 2 Batch 57/179 Loss: 0.006177366245537996\n",
            "Epoch 2 Batch 58/179 Loss: 0.07003933191299438\n",
            "Epoch 2 Batch 59/179 Loss: 0.018402820453047752\n",
            "Epoch 2 Batch 60/179 Loss: 0.005053172819316387\n",
            "Epoch 2 Batch 61/179 Loss: 0.022495266050100327\n",
            "Epoch 2 Batch 62/179 Loss: 0.010666915215551853\n",
            "Epoch 2 Batch 63/179 Loss: 0.015539323911070824\n",
            "Epoch 2 Batch 64/179 Loss: 0.029989298433065414\n",
            "Epoch 2 Batch 65/179 Loss: 0.08207075297832489\n",
            "Epoch 2 Batch 66/179 Loss: 0.04946553707122803\n",
            "Epoch 2 Batch 67/179 Loss: 0.0077553801238536835\n",
            "Epoch 2 Batch 68/179 Loss: 0.008967120200395584\n",
            "Epoch 2 Batch 69/179 Loss: 0.00895174965262413\n",
            "Epoch 2 Batch 70/179 Loss: 0.00863325223326683\n",
            "Epoch 2 Batch 71/179 Loss: 0.011776014231145382\n",
            "Epoch 2 Batch 72/179 Loss: 0.007383683230727911\n",
            "Epoch 2 Batch 73/179 Loss: 0.07978075742721558\n",
            "Epoch 2 Batch 74/179 Loss: 0.007967885583639145\n",
            "Epoch 2 Batch 75/179 Loss: 0.025358524173498154\n",
            "Epoch 2 Batch 76/179 Loss: 0.014212638139724731\n",
            "Epoch 2 Batch 77/179 Loss: 0.01950101926922798\n",
            "Epoch 2 Batch 78/179 Loss: 0.009202554821968079\n",
            "Epoch 2 Batch 79/179 Loss: 0.019830655306577682\n",
            "Epoch 2 Batch 80/179 Loss: 0.03765860199928284\n",
            "Epoch 2 Batch 81/179 Loss: 0.03386073186993599\n",
            "Epoch 2 Batch 82/179 Loss: 0.0012558074668049812\n",
            "Epoch 2 Batch 83/179 Loss: 0.02516118995845318\n",
            "Epoch 2 Batch 84/179 Loss: 0.020096128806471825\n",
            "Epoch 2 Batch 85/179 Loss: 0.02706541121006012\n",
            "Epoch 2 Batch 86/179 Loss: 0.09014949202537537\n",
            "Epoch 2 Batch 87/179 Loss: 0.09902974218130112\n",
            "Epoch 2 Batch 88/179 Loss: 0.022851206362247467\n",
            "Epoch 2 Batch 89/179 Loss: 0.010757091455161572\n",
            "Epoch 2 Batch 90/179 Loss: 0.01321165170520544\n",
            "Epoch 2 Batch 91/179 Loss: 0.06455236673355103\n",
            "Epoch 2 Batch 92/179 Loss: 0.02286616899073124\n",
            "Epoch 2 Batch 93/179 Loss: 0.013911081477999687\n",
            "Epoch 2 Batch 94/179 Loss: 0.0018284703837707639\n",
            "Epoch 2 Batch 95/179 Loss: 0.034968528896570206\n",
            "Epoch 2 Batch 96/179 Loss: 0.021593226119875908\n",
            "Epoch 2 Batch 97/179 Loss: 0.009446526877582073\n",
            "Epoch 2 Batch 98/179 Loss: 0.014257583767175674\n",
            "Epoch 2 Batch 99/179 Loss: 0.027923405170440674\n",
            "Epoch 2 Batch 100/179 Loss: 0.007646727841347456\n",
            "Epoch 2 Batch 101/179 Loss: 0.008744286373257637\n",
            "Epoch 2 Batch 102/179 Loss: 0.06383481621742249\n",
            "Epoch 2 Batch 103/179 Loss: 0.034979261457920074\n",
            "Epoch 2 Batch 104/179 Loss: 0.11055205017328262\n",
            "Epoch 2 Batch 105/179 Loss: 0.03491726517677307\n",
            "Epoch 2 Batch 106/179 Loss: 0.011112755164504051\n",
            "Epoch 2 Batch 107/179 Loss: 0.018879741430282593\n",
            "Epoch 2 Batch 108/179 Loss: 0.0021885058376938105\n",
            "Epoch 2 Batch 109/179 Loss: 0.050110720098018646\n",
            "Epoch 2 Batch 110/179 Loss: 0.00495639443397522\n",
            "Epoch 2 Batch 111/179 Loss: 0.016208136454224586\n",
            "Epoch 2 Batch 112/179 Loss: 0.05408959090709686\n",
            "Epoch 2 Batch 113/179 Loss: 0.025706544518470764\n",
            "Epoch 2 Batch 114/179 Loss: 0.014169503003358841\n",
            "Epoch 2 Batch 115/179 Loss: 0.01439726259559393\n",
            "Epoch 2 Batch 116/179 Loss: 0.02214423194527626\n",
            "Epoch 2 Batch 117/179 Loss: 0.02142840251326561\n",
            "Epoch 2 Batch 118/179 Loss: 0.007204133551567793\n",
            "Epoch 2 Batch 119/179 Loss: 0.009977325797080994\n",
            "Epoch 2 Batch 120/179 Loss: 0.005052618682384491\n",
            "Epoch 2 Batch 121/179 Loss: 0.022374585270881653\n",
            "Epoch 2 Batch 122/179 Loss: 0.014502913691103458\n",
            "Epoch 2 Batch 123/179 Loss: 0.12926259636878967\n",
            "Epoch 2 Batch 124/179 Loss: 0.004932059440761805\n",
            "Epoch 2 Batch 125/179 Loss: 0.03333744779229164\n",
            "Epoch 2 Batch 126/179 Loss: 0.016062315553426743\n",
            "Epoch 2 Batch 127/179 Loss: 0.044280845671892166\n",
            "Epoch 2 Batch 128/179 Loss: 0.017673935741186142\n",
            "Epoch 2 Batch 129/179 Loss: 0.004305979236960411\n",
            "Epoch 2 Batch 130/179 Loss: 0.01506861113011837\n",
            "Epoch 2 Batch 131/179 Loss: 0.008860679343342781\n",
            "Epoch 2 Batch 132/179 Loss: 0.011722957715392113\n",
            "Epoch 2 Batch 133/179 Loss: 0.013434495776891708\n",
            "Epoch 2 Batch 134/179 Loss: 0.03401681035757065\n",
            "Epoch 2 Batch 135/179 Loss: 0.027685698121786118\n",
            "Epoch 2 Batch 136/179 Loss: 0.06205527111887932\n",
            "Epoch 2 Batch 137/179 Loss: 0.022590218111872673\n",
            "Epoch 2 Batch 138/179 Loss: 0.02464277297258377\n",
            "Epoch 2 Batch 139/179 Loss: 0.024156633764505386\n",
            "Epoch 2 Batch 140/179 Loss: 0.2010895162820816\n",
            "Epoch 2 Batch 141/179 Loss: 0.11814402043819427\n",
            "Epoch 2 Batch 142/179 Loss: 0.08779793232679367\n",
            "Epoch 2 Batch 143/179 Loss: 0.013511085882782936\n",
            "Epoch 2 Batch 144/179 Loss: 0.0496305413544178\n",
            "Epoch 2 Batch 145/179 Loss: 0.1842074692249298\n",
            "Epoch 2 Batch 146/179 Loss: 0.04315214976668358\n",
            "Epoch 2 Batch 147/179 Loss: 0.03273341804742813\n",
            "Epoch 2 Batch 148/179 Loss: 0.0245194174349308\n",
            "Epoch 2 Batch 149/179 Loss: 0.04492103308439255\n",
            "Epoch 2 Batch 150/179 Loss: 0.04598649963736534\n",
            "Epoch 2 Batch 151/179 Loss: 0.1443357914686203\n",
            "Epoch 2 Batch 152/179 Loss: 0.00881282240152359\n",
            "Epoch 2 Batch 153/179 Loss: 0.10959723591804504\n",
            "Epoch 2 Batch 154/179 Loss: 0.0116222919896245\n",
            "Epoch 2 Batch 155/179 Loss: 0.07876771688461304\n",
            "Epoch 2 Batch 156/179 Loss: 0.015179606154561043\n",
            "Epoch 2 Batch 157/179 Loss: 0.004311501048505306\n",
            "Epoch 2 Batch 158/179 Loss: 0.01583554968237877\n",
            "Epoch 2 Batch 159/179 Loss: 0.006709759589284658\n",
            "Epoch 2 Batch 160/179 Loss: 0.0210280679166317\n",
            "Epoch 2 Batch 161/179 Loss: 0.020237579941749573\n",
            "Epoch 2 Batch 162/179 Loss: 0.021984413266181946\n",
            "Epoch 2 Batch 163/179 Loss: 0.010337460786104202\n",
            "Epoch 2 Batch 164/179 Loss: 0.003214683849364519\n",
            "Epoch 2 Batch 165/179 Loss: 0.04116009175777435\n",
            "Epoch 2 Batch 166/179 Loss: 0.00870513916015625\n",
            "Epoch 2 Batch 167/179 Loss: 0.08778782933950424\n",
            "Epoch 2 Batch 168/179 Loss: 0.06188300624489784\n",
            "Epoch 2 Batch 169/179 Loss: 0.006030097138136625\n",
            "Epoch 2 Batch 170/179 Loss: 0.006266693584620953\n",
            "Epoch 2 Batch 171/179 Loss: 0.007650189567357302\n",
            "Epoch 2 Batch 172/179 Loss: 0.07819677889347076\n",
            "Epoch 2 Batch 173/179 Loss: 0.010328062810003757\n",
            "Epoch 2 Batch 174/179 Loss: 0.025320619344711304\n",
            "Epoch 2 Batch 175/179 Loss: 0.005685770884156227\n",
            "Epoch 2 Batch 176/179 Loss: 0.006204875651746988\n",
            "Epoch 2 Batch 177/179 Loss: 0.0688108578324318\n",
            "Epoch 2 Batch 178/179 Loss: 0.028180569410324097\n",
            "Epoch 2 Batch 179/179 Loss: 0.04289836436510086\n",
            "Epoch 2/10 Avg Loss: 0.03233934192223651\n",
            "Epoch 3 Batch 1/179 Loss: 0.04039270058274269\n",
            "Epoch 3 Batch 2/179 Loss: 0.020957551896572113\n",
            "Epoch 3 Batch 3/179 Loss: 0.06531140208244324\n",
            "Epoch 3 Batch 4/179 Loss: 0.044883593916893005\n",
            "Epoch 3 Batch 5/179 Loss: 0.02308354713022709\n",
            "Epoch 3 Batch 6/179 Loss: 0.005075174383819103\n",
            "Epoch 3 Batch 7/179 Loss: 0.03199797123670578\n",
            "Epoch 3 Batch 8/179 Loss: 0.004764053039252758\n",
            "Epoch 3 Batch 9/179 Loss: 0.04411487653851509\n",
            "Epoch 3 Batch 10/179 Loss: 0.041959360241889954\n",
            "Epoch 3 Batch 11/179 Loss: 0.01293743122369051\n",
            "Epoch 3 Batch 12/179 Loss: 0.009722454473376274\n",
            "Epoch 3 Batch 13/179 Loss: 0.04453564062714577\n",
            "Epoch 3 Batch 14/179 Loss: 0.024369079619646072\n",
            "Epoch 3 Batch 15/179 Loss: 0.03334265947341919\n",
            "Epoch 3 Batch 16/179 Loss: 0.012634028680622578\n",
            "Epoch 3 Batch 17/179 Loss: 0.036991655826568604\n",
            "Epoch 3 Batch 18/179 Loss: 0.032786499708890915\n",
            "Epoch 3 Batch 19/179 Loss: 0.0148753821849823\n",
            "Epoch 3 Batch 20/179 Loss: 0.006905367597937584\n",
            "Epoch 3 Batch 21/179 Loss: 0.00737271923571825\n",
            "Epoch 3 Batch 22/179 Loss: 0.016253329813480377\n",
            "Epoch 3 Batch 23/179 Loss: 0.005122884176671505\n",
            "Epoch 3 Batch 24/179 Loss: 0.0071393572725355625\n",
            "Epoch 3 Batch 25/179 Loss: 0.014555178582668304\n",
            "Epoch 3 Batch 26/179 Loss: 0.028618987649679184\n",
            "Epoch 3 Batch 27/179 Loss: 0.02744527906179428\n",
            "Epoch 3 Batch 28/179 Loss: 0.0032680141739547253\n",
            "Epoch 3 Batch 29/179 Loss: 0.0017679918091744184\n",
            "Epoch 3 Batch 30/179 Loss: 0.02372824400663376\n",
            "Epoch 3 Batch 31/179 Loss: 0.017238810658454895\n",
            "Epoch 3 Batch 32/179 Loss: 0.1299431174993515\n",
            "Epoch 3 Batch 33/179 Loss: 0.09372736513614655\n",
            "Epoch 3 Batch 34/179 Loss: 0.05485676974058151\n",
            "Epoch 3 Batch 35/179 Loss: 0.007791141979396343\n",
            "Epoch 3 Batch 36/179 Loss: 0.013412971049547195\n",
            "Epoch 3 Batch 37/179 Loss: 0.02811277285218239\n",
            "Epoch 3 Batch 38/179 Loss: 0.04167695343494415\n",
            "Epoch 3 Batch 39/179 Loss: 0.016838327050209045\n",
            "Epoch 3 Batch 40/179 Loss: 0.00476752407848835\n",
            "Epoch 3 Batch 41/179 Loss: 0.036166366189718246\n",
            "Epoch 3 Batch 42/179 Loss: 0.005759809166193008\n",
            "Epoch 3 Batch 43/179 Loss: 0.03451886028051376\n",
            "Epoch 3 Batch 44/179 Loss: 0.008668963797390461\n",
            "Epoch 3 Batch 45/179 Loss: 0.037670399993658066\n",
            "Epoch 3 Batch 46/179 Loss: 0.042849212884902954\n",
            "Epoch 3 Batch 47/179 Loss: 0.013853205367922783\n",
            "Epoch 3 Batch 48/179 Loss: 0.02085089683532715\n",
            "Epoch 3 Batch 49/179 Loss: 0.023223066702485085\n",
            "Epoch 3 Batch 50/179 Loss: 0.02653549611568451\n",
            "Epoch 3 Batch 51/179 Loss: 0.00976619217544794\n",
            "Epoch 3 Batch 52/179 Loss: 0.013944693841040134\n",
            "Epoch 3 Batch 53/179 Loss: 0.018488310277462006\n",
            "Epoch 3 Batch 54/179 Loss: 0.008724781684577465\n",
            "Epoch 3 Batch 55/179 Loss: 0.07075968384742737\n",
            "Epoch 3 Batch 56/179 Loss: 0.012595168314874172\n",
            "Epoch 3 Batch 57/179 Loss: 0.006983853876590729\n",
            "Epoch 3 Batch 58/179 Loss: 0.010602127760648727\n",
            "Epoch 3 Batch 59/179 Loss: 0.027988074347376823\n",
            "Epoch 3 Batch 60/179 Loss: 0.01023428700864315\n",
            "Epoch 3 Batch 61/179 Loss: 0.03711112216114998\n",
            "Epoch 3 Batch 62/179 Loss: 0.034873899072408676\n",
            "Epoch 3 Batch 63/179 Loss: 0.0037734087090939283\n",
            "Epoch 3 Batch 64/179 Loss: 0.003807581029832363\n",
            "Epoch 3 Batch 65/179 Loss: 0.05257265269756317\n",
            "Epoch 3 Batch 66/179 Loss: 0.013043484650552273\n",
            "Epoch 3 Batch 67/179 Loss: 0.012299936264753342\n",
            "Epoch 3 Batch 68/179 Loss: 0.007660450413823128\n",
            "Epoch 3 Batch 69/179 Loss: 0.02212238684296608\n",
            "Epoch 3 Batch 70/179 Loss: 0.0540178045630455\n",
            "Epoch 3 Batch 71/179 Loss: 0.029811706393957138\n",
            "Epoch 3 Batch 72/179 Loss: 0.02423016168177128\n",
            "Epoch 3 Batch 73/179 Loss: 0.009083411656320095\n",
            "Epoch 3 Batch 74/179 Loss: 0.010354142636060715\n",
            "Epoch 3 Batch 75/179 Loss: 0.03179706633090973\n",
            "Epoch 3 Batch 76/179 Loss: 0.054528024047613144\n",
            "Epoch 3 Batch 77/179 Loss: 0.002640683203935623\n",
            "Epoch 3 Batch 78/179 Loss: 0.016776159405708313\n",
            "Epoch 3 Batch 79/179 Loss: 0.005675403401255608\n",
            "Epoch 3 Batch 80/179 Loss: 0.0426081158220768\n",
            "Epoch 3 Batch 81/179 Loss: 0.014686264097690582\n",
            "Epoch 3 Batch 82/179 Loss: 0.007100098766386509\n",
            "Epoch 3 Batch 83/179 Loss: 0.0076146433129906654\n",
            "Epoch 3 Batch 84/179 Loss: 0.04012272134423256\n",
            "Epoch 3 Batch 85/179 Loss: 0.030194196850061417\n",
            "Epoch 3 Batch 86/179 Loss: 0.13903144001960754\n",
            "Epoch 3 Batch 87/179 Loss: 0.0512038879096508\n",
            "Epoch 3 Batch 88/179 Loss: 0.008845871314406395\n",
            "Epoch 3 Batch 89/179 Loss: 0.009228300303220749\n",
            "Epoch 3 Batch 90/179 Loss: 0.005853545852005482\n",
            "Epoch 3 Batch 91/179 Loss: 0.1024644672870636\n",
            "Epoch 3 Batch 92/179 Loss: 0.037138115614652634\n",
            "Epoch 3 Batch 93/179 Loss: 0.04147491976618767\n",
            "Epoch 3 Batch 94/179 Loss: 0.009667305275797844\n",
            "Epoch 3 Batch 95/179 Loss: 0.04282734915614128\n",
            "Epoch 3 Batch 96/179 Loss: 0.00978116039186716\n",
            "Epoch 3 Batch 97/179 Loss: 0.020143207162618637\n",
            "Epoch 3 Batch 98/179 Loss: 0.004885688424110413\n",
            "Epoch 3 Batch 99/179 Loss: 0.0039203595370054245\n",
            "Epoch 3 Batch 100/179 Loss: 0.03178643062710762\n",
            "Epoch 3 Batch 101/179 Loss: 0.026339566335082054\n",
            "Epoch 3 Batch 102/179 Loss: 0.012049037963151932\n",
            "Epoch 3 Batch 103/179 Loss: 0.008785361424088478\n",
            "Epoch 3 Batch 104/179 Loss: 0.1442485749721527\n",
            "Epoch 3 Batch 105/179 Loss: 0.026018140837550163\n",
            "Epoch 3 Batch 106/179 Loss: 0.020350631326436996\n",
            "Epoch 3 Batch 107/179 Loss: 0.009803280234336853\n",
            "Epoch 3 Batch 108/179 Loss: 0.006527346093207598\n",
            "Epoch 3 Batch 109/179 Loss: 0.06844774633646011\n",
            "Epoch 3 Batch 110/179 Loss: 0.013654312118887901\n",
            "Epoch 3 Batch 111/179 Loss: 0.003542059101164341\n",
            "Epoch 3 Batch 112/179 Loss: 0.005529667250812054\n",
            "Epoch 3 Batch 113/179 Loss: 0.03533367067575455\n",
            "Epoch 3 Batch 114/179 Loss: 0.00355822523124516\n",
            "Epoch 3 Batch 115/179 Loss: 0.15419615805149078\n",
            "Epoch 3 Batch 116/179 Loss: 0.020866194739937782\n",
            "Epoch 3 Batch 117/179 Loss: 0.12873218953609467\n",
            "Epoch 3 Batch 118/179 Loss: 0.014753622934222221\n",
            "Epoch 3 Batch 119/179 Loss: 0.004987172782421112\n",
            "Epoch 3 Batch 120/179 Loss: 0.12174241244792938\n",
            "Epoch 3 Batch 121/179 Loss: 0.00868331827223301\n",
            "Epoch 3 Batch 122/179 Loss: 0.021795962005853653\n",
            "Epoch 3 Batch 123/179 Loss: 0.010256050154566765\n",
            "Epoch 3 Batch 124/179 Loss: 0.010649386793375015\n",
            "Epoch 3 Batch 125/179 Loss: 0.0296215508133173\n",
            "Epoch 3 Batch 126/179 Loss: 0.008791312575340271\n",
            "Epoch 3 Batch 127/179 Loss: 0.01604556292295456\n",
            "Epoch 3 Batch 128/179 Loss: 0.011810347437858582\n",
            "Epoch 3 Batch 129/179 Loss: 0.0006789480103179812\n",
            "Epoch 3 Batch 130/179 Loss: 0.01892576366662979\n",
            "Epoch 3 Batch 131/179 Loss: 0.006773380097001791\n",
            "Epoch 3 Batch 132/179 Loss: 0.11197807639837265\n",
            "Epoch 3 Batch 133/179 Loss: 0.019766755402088165\n",
            "Epoch 3 Batch 134/179 Loss: 0.04052369296550751\n",
            "Epoch 3 Batch 135/179 Loss: 0.018380269408226013\n",
            "Epoch 3 Batch 136/179 Loss: 0.006653858348727226\n",
            "Epoch 3 Batch 137/179 Loss: 0.009143753908574581\n",
            "Epoch 3 Batch 138/179 Loss: 0.013585496693849564\n",
            "Epoch 3 Batch 139/179 Loss: 0.0070627788081765175\n",
            "Epoch 3 Batch 140/179 Loss: 0.017207331955432892\n",
            "Epoch 3 Batch 141/179 Loss: 0.038394857197999954\n",
            "Epoch 3 Batch 142/179 Loss: 0.12877126038074493\n",
            "Epoch 3 Batch 143/179 Loss: 0.006412523798644543\n",
            "Epoch 3 Batch 144/179 Loss: 0.03050207532942295\n",
            "Epoch 3 Batch 145/179 Loss: 0.041602808982133865\n",
            "Epoch 3 Batch 146/179 Loss: 0.010894403792917728\n",
            "Epoch 3 Batch 147/179 Loss: 0.018971240147948265\n",
            "Epoch 3 Batch 148/179 Loss: 0.00809641182422638\n",
            "Epoch 3 Batch 149/179 Loss: 0.056622397154569626\n",
            "Epoch 3 Batch 150/179 Loss: 0.011100814677774906\n",
            "Epoch 3 Batch 151/179 Loss: 0.0034774623345583677\n",
            "Epoch 3 Batch 152/179 Loss: 0.006218127906322479\n",
            "Epoch 3 Batch 153/179 Loss: 0.01178075559437275\n",
            "Epoch 3 Batch 154/179 Loss: 0.010592421516776085\n",
            "Epoch 3 Batch 155/179 Loss: 0.030442330986261368\n",
            "Epoch 3 Batch 156/179 Loss: 0.08150206506252289\n",
            "Epoch 3 Batch 157/179 Loss: 0.008934627287089825\n",
            "Epoch 3 Batch 158/179 Loss: 0.07951971888542175\n",
            "Epoch 3 Batch 159/179 Loss: 0.03502550721168518\n",
            "Epoch 3 Batch 160/179 Loss: 0.010512792505323887\n",
            "Epoch 3 Batch 161/179 Loss: 0.0267238337546587\n",
            "Epoch 3 Batch 162/179 Loss: 0.02645733207464218\n",
            "Epoch 3 Batch 163/179 Loss: 0.023625768721103668\n",
            "Epoch 3 Batch 164/179 Loss: 0.008568322286009789\n",
            "Epoch 3 Batch 165/179 Loss: 0.03964938223361969\n",
            "Epoch 3 Batch 166/179 Loss: 0.011835502460598946\n",
            "Epoch 3 Batch 167/179 Loss: 0.03961314260959625\n",
            "Epoch 3 Batch 168/179 Loss: 0.052997931838035583\n",
            "Epoch 3 Batch 169/179 Loss: 0.024323243647813797\n",
            "Epoch 3 Batch 170/179 Loss: 0.0698283240199089\n",
            "Epoch 3 Batch 171/179 Loss: 0.054827287793159485\n",
            "Epoch 3 Batch 172/179 Loss: 0.01630784198641777\n",
            "Epoch 3 Batch 173/179 Loss: 0.01230144314467907\n",
            "Epoch 3 Batch 174/179 Loss: 0.05987975001335144\n",
            "Epoch 3 Batch 175/179 Loss: 0.04149426892399788\n",
            "Epoch 3 Batch 176/179 Loss: 0.05456329137086868\n",
            "Epoch 3 Batch 177/179 Loss: 0.020222598686814308\n",
            "Epoch 3 Batch 178/179 Loss: 0.0872659981250763\n",
            "Epoch 3 Batch 179/179 Loss: 0.0005214036791585386\n",
            "Epoch 3/10 Avg Loss: 0.02871731525793016\n",
            "Epoch 4 Batch 1/179 Loss: 0.009002323262393475\n",
            "Epoch 4 Batch 2/179 Loss: 0.0032319077290594578\n",
            "Epoch 4 Batch 3/179 Loss: 0.010961070656776428\n",
            "Epoch 4 Batch 4/179 Loss: 0.021590156480669975\n",
            "Epoch 4 Batch 5/179 Loss: 0.031813737004995346\n",
            "Epoch 4 Batch 6/179 Loss: 0.09334598481655121\n",
            "Epoch 4 Batch 7/179 Loss: 0.009953535161912441\n",
            "Epoch 4 Batch 8/179 Loss: 0.01469340082257986\n",
            "Epoch 4 Batch 9/179 Loss: 0.007263443432748318\n",
            "Epoch 4 Batch 10/179 Loss: 0.01110849529504776\n",
            "Epoch 4 Batch 11/179 Loss: 0.005469565745443106\n",
            "Epoch 4 Batch 12/179 Loss: 0.006729527842253447\n",
            "Epoch 4 Batch 13/179 Loss: 0.026914505288004875\n",
            "Epoch 4 Batch 14/179 Loss: 0.03057452291250229\n",
            "Epoch 4 Batch 15/179 Loss: 0.017990993335843086\n",
            "Epoch 4 Batch 16/179 Loss: 0.03700769320130348\n",
            "Epoch 4 Batch 17/179 Loss: 0.04468359053134918\n",
            "Epoch 4 Batch 18/179 Loss: 0.0013412958942353725\n",
            "Epoch 4 Batch 19/179 Loss: 0.005350393243134022\n",
            "Epoch 4 Batch 20/179 Loss: 0.008965003304183483\n",
            "Epoch 4 Batch 21/179 Loss: 0.005781105719506741\n",
            "Epoch 4 Batch 22/179 Loss: 0.04956459254026413\n",
            "Epoch 4 Batch 23/179 Loss: 0.05046014487743378\n",
            "Epoch 4 Batch 24/179 Loss: 0.010329646058380604\n",
            "Epoch 4 Batch 25/179 Loss: 0.007900404743850231\n",
            "Epoch 4 Batch 26/179 Loss: 0.019614621996879578\n",
            "Epoch 4 Batch 27/179 Loss: 0.004552729893475771\n",
            "Epoch 4 Batch 28/179 Loss: 0.00968819111585617\n",
            "Epoch 4 Batch 29/179 Loss: 0.001305318670347333\n",
            "Epoch 4 Batch 30/179 Loss: 0.010674466378986835\n",
            "Epoch 4 Batch 31/179 Loss: 0.01493865717202425\n",
            "Epoch 4 Batch 32/179 Loss: 0.026190998032689095\n",
            "Epoch 4 Batch 33/179 Loss: 0.018829800188541412\n",
            "Epoch 4 Batch 34/179 Loss: 0.0027092378586530685\n",
            "Epoch 4 Batch 35/179 Loss: 0.01051255501806736\n",
            "Epoch 4 Batch 36/179 Loss: 0.0418059416115284\n",
            "Epoch 4 Batch 37/179 Loss: 0.029125578701496124\n",
            "Epoch 4 Batch 38/179 Loss: 0.013930714689195156\n",
            "Epoch 4 Batch 39/179 Loss: 0.004523512441664934\n",
            "Epoch 4 Batch 40/179 Loss: 0.1176624745130539\n",
            "Epoch 4 Batch 41/179 Loss: 0.04685214161872864\n",
            "Epoch 4 Batch 42/179 Loss: 0.013007549569010735\n",
            "Epoch 4 Batch 43/179 Loss: 0.021581321954727173\n",
            "Epoch 4 Batch 44/179 Loss: 0.006146423518657684\n",
            "Epoch 4 Batch 45/179 Loss: 0.007086414843797684\n",
            "Epoch 4 Batch 46/179 Loss: 0.07147436589002609\n",
            "Epoch 4 Batch 47/179 Loss: 0.03752617910504341\n",
            "Epoch 4 Batch 48/179 Loss: 0.0306234173476696\n",
            "Epoch 4 Batch 49/179 Loss: 0.006669101770967245\n",
            "Epoch 4 Batch 50/179 Loss: 0.011474095284938812\n",
            "Epoch 4 Batch 51/179 Loss: 0.008383484557271004\n",
            "Epoch 4 Batch 52/179 Loss: 0.01718185283243656\n",
            "Epoch 4 Batch 53/179 Loss: 0.008833755739033222\n",
            "Epoch 4 Batch 54/179 Loss: 0.13568559288978577\n",
            "Epoch 4 Batch 55/179 Loss: 0.1035887822508812\n",
            "Epoch 4 Batch 56/179 Loss: 0.02838895097374916\n",
            "Epoch 4 Batch 57/179 Loss: 0.002908327616751194\n",
            "Epoch 4 Batch 58/179 Loss: 0.006588311400264502\n",
            "Epoch 4 Batch 59/179 Loss: 0.0074834078550338745\n",
            "Epoch 4 Batch 60/179 Loss: 0.039402179419994354\n",
            "Epoch 4 Batch 61/179 Loss: 0.028373654931783676\n",
            "Epoch 4 Batch 62/179 Loss: 0.03620311617851257\n",
            "Epoch 4 Batch 63/179 Loss: 0.0009704264812171459\n",
            "Epoch 4 Batch 64/179 Loss: 0.06737259775400162\n",
            "Epoch 4 Batch 65/179 Loss: 0.035128284245729446\n",
            "Epoch 4 Batch 66/179 Loss: 0.011299163103103638\n",
            "Epoch 4 Batch 67/179 Loss: 0.010096592828631401\n",
            "Epoch 4 Batch 68/179 Loss: 0.03933332860469818\n",
            "Epoch 4 Batch 69/179 Loss: 0.007349906489253044\n",
            "Epoch 4 Batch 70/179 Loss: 0.00471224170178175\n",
            "Epoch 4 Batch 71/179 Loss: 0.024380268529057503\n",
            "Epoch 4 Batch 72/179 Loss: 0.008578063920140266\n",
            "Epoch 4 Batch 73/179 Loss: 0.02490684762597084\n",
            "Epoch 4 Batch 74/179 Loss: 0.0043168384581804276\n",
            "Epoch 4 Batch 75/179 Loss: 0.002538606757298112\n",
            "Epoch 4 Batch 76/179 Loss: 0.01658284291625023\n",
            "Epoch 4 Batch 77/179 Loss: 0.001701079891063273\n",
            "Epoch 4 Batch 78/179 Loss: 0.0532398447394371\n",
            "Epoch 4 Batch 79/179 Loss: 0.0576709546148777\n",
            "Epoch 4 Batch 80/179 Loss: 0.027723295614123344\n",
            "Epoch 4 Batch 81/179 Loss: 0.02814885601401329\n",
            "Epoch 4 Batch 82/179 Loss: 0.010599827393889427\n",
            "Epoch 4 Batch 83/179 Loss: 0.01555909775197506\n",
            "Epoch 4 Batch 84/179 Loss: 0.033779919147491455\n",
            "Epoch 4 Batch 85/179 Loss: 0.011654691770672798\n",
            "Epoch 4 Batch 86/179 Loss: 0.019246965646743774\n",
            "Epoch 4 Batch 87/179 Loss: 0.013707800768315792\n",
            "Epoch 4 Batch 88/179 Loss: 0.028401311486959457\n",
            "Epoch 4 Batch 89/179 Loss: 0.007009563036262989\n",
            "Epoch 4 Batch 90/179 Loss: 0.02323223277926445\n",
            "Epoch 4 Batch 91/179 Loss: 0.07262782752513885\n",
            "Epoch 4 Batch 92/179 Loss: 0.07013736665248871\n",
            "Epoch 4 Batch 93/179 Loss: 0.005088677629828453\n",
            "Epoch 4 Batch 94/179 Loss: 0.012153505347669125\n",
            "Epoch 4 Batch 95/179 Loss: 0.0006191869033500552\n",
            "Epoch 4 Batch 96/179 Loss: 0.041470207273960114\n",
            "Epoch 4 Batch 97/179 Loss: 0.004974294453859329\n",
            "Epoch 4 Batch 98/179 Loss: 0.07449854165315628\n",
            "Epoch 4 Batch 99/179 Loss: 0.08888376504182816\n",
            "Epoch 4 Batch 100/179 Loss: 0.021935569122433662\n",
            "Epoch 4 Batch 101/179 Loss: 0.004472164437174797\n",
            "Epoch 4 Batch 102/179 Loss: 0.008067088201642036\n",
            "Epoch 4 Batch 103/179 Loss: 0.07617639750242233\n",
            "Epoch 4 Batch 104/179 Loss: 0.011214355006814003\n",
            "Epoch 4 Batch 105/179 Loss: 0.033572807908058167\n",
            "Epoch 4 Batch 106/179 Loss: 0.04517490416765213\n",
            "Epoch 4 Batch 107/179 Loss: 0.0511624738574028\n",
            "Epoch 4 Batch 108/179 Loss: 0.014361121691763401\n",
            "Epoch 4 Batch 109/179 Loss: 0.006609632167965174\n",
            "Epoch 4 Batch 110/179 Loss: 0.010894439183175564\n",
            "Epoch 4 Batch 111/179 Loss: 0.008557628840208054\n",
            "Epoch 4 Batch 112/179 Loss: 0.008346905931830406\n",
            "Epoch 4 Batch 113/179 Loss: 0.14141060411930084\n",
            "Epoch 4 Batch 114/179 Loss: 0.03652263805270195\n",
            "Epoch 4 Batch 115/179 Loss: 0.016288939863443375\n",
            "Epoch 4 Batch 116/179 Loss: 0.010404533706605434\n",
            "Epoch 4 Batch 117/179 Loss: 0.032322321087121964\n",
            "Epoch 4 Batch 118/179 Loss: 0.027409221976995468\n",
            "Epoch 4 Batch 119/179 Loss: 0.009203838184475899\n",
            "Epoch 4 Batch 120/179 Loss: 0.017465835437178612\n",
            "Epoch 4 Batch 121/179 Loss: 0.029596013948321342\n",
            "Epoch 4 Batch 122/179 Loss: 0.004742152988910675\n",
            "Epoch 4 Batch 123/179 Loss: 0.020246068015694618\n",
            "Epoch 4 Batch 124/179 Loss: 0.04407530650496483\n",
            "Epoch 4 Batch 125/179 Loss: 0.04112328961491585\n",
            "Epoch 4 Batch 126/179 Loss: 0.02416597492992878\n",
            "Epoch 4 Batch 127/179 Loss: 0.0023390627466142178\n",
            "Epoch 4 Batch 128/179 Loss: 0.03154485300183296\n",
            "Epoch 4 Batch 129/179 Loss: 0.024480555206537247\n",
            "Epoch 4 Batch 130/179 Loss: 0.0028249395545572042\n",
            "Epoch 4 Batch 131/179 Loss: 0.011294840835034847\n",
            "Epoch 4 Batch 132/179 Loss: 0.04903394356369972\n",
            "Epoch 4 Batch 133/179 Loss: 0.11661264300346375\n",
            "Epoch 4 Batch 134/179 Loss: 0.04644519090652466\n",
            "Epoch 4 Batch 135/179 Loss: 0.002877733903005719\n",
            "Epoch 4 Batch 136/179 Loss: 0.018381454050540924\n",
            "Epoch 4 Batch 137/179 Loss: 0.03814052417874336\n",
            "Epoch 4 Batch 138/179 Loss: 0.008434844203293324\n",
            "Epoch 4 Batch 139/179 Loss: 0.011460823938250542\n",
            "Epoch 4 Batch 140/179 Loss: 0.0030457801185548306\n",
            "Epoch 4 Batch 141/179 Loss: 0.003124568145722151\n",
            "Epoch 4 Batch 142/179 Loss: 0.03708449378609657\n",
            "Epoch 4 Batch 143/179 Loss: 0.0037727435119450092\n",
            "Epoch 4 Batch 144/179 Loss: 0.023812780156731606\n",
            "Epoch 4 Batch 145/179 Loss: 0.01316741481423378\n",
            "Epoch 4 Batch 146/179 Loss: 0.0031238263472914696\n",
            "Epoch 4 Batch 147/179 Loss: 0.01251418236643076\n",
            "Epoch 4 Batch 148/179 Loss: 0.0554811954498291\n",
            "Epoch 4 Batch 149/179 Loss: 0.03917679935693741\n",
            "Epoch 4 Batch 150/179 Loss: 0.02384142577648163\n",
            "Epoch 4 Batch 151/179 Loss: 0.002385258674621582\n",
            "Epoch 4 Batch 152/179 Loss: 0.03624526411294937\n",
            "Epoch 4 Batch 153/179 Loss: 0.02300719916820526\n",
            "Epoch 4 Batch 154/179 Loss: 0.12197793275117874\n",
            "Epoch 4 Batch 155/179 Loss: 0.004337309394031763\n",
            "Epoch 4 Batch 156/179 Loss: 0.015122652985155582\n",
            "Epoch 4 Batch 157/179 Loss: 0.011057084426283836\n",
            "Epoch 4 Batch 158/179 Loss: 0.0069158636033535\n",
            "Epoch 4 Batch 159/179 Loss: 0.009770790114998817\n",
            "Epoch 4 Batch 160/179 Loss: 0.03985491022467613\n",
            "Epoch 4 Batch 161/179 Loss: 0.021582772955298424\n",
            "Epoch 4 Batch 162/179 Loss: 0.026423651725053787\n",
            "Epoch 4 Batch 163/179 Loss: 0.12933757901191711\n",
            "Epoch 4 Batch 164/179 Loss: 0.013689479790627956\n",
            "Epoch 4 Batch 165/179 Loss: 0.00768413208425045\n",
            "Epoch 4 Batch 166/179 Loss: 0.016103025525808334\n",
            "Epoch 4 Batch 167/179 Loss: 0.05904412642121315\n",
            "Epoch 4 Batch 168/179 Loss: 0.048989713191986084\n",
            "Epoch 4 Batch 169/179 Loss: 0.02462904341518879\n",
            "Epoch 4 Batch 170/179 Loss: 0.03166159987449646\n",
            "Epoch 4 Batch 171/179 Loss: 0.002560928463935852\n",
            "Epoch 4 Batch 172/179 Loss: 0.008899876847863197\n",
            "Epoch 4 Batch 173/179 Loss: 0.049235694110393524\n",
            "Epoch 4 Batch 174/179 Loss: 0.01564495638012886\n",
            "Epoch 4 Batch 175/179 Loss: 0.00253050634637475\n",
            "Epoch 4 Batch 176/179 Loss: 0.07485049217939377\n",
            "Epoch 4 Batch 177/179 Loss: 0.010440287180244923\n",
            "Epoch 4 Batch 178/179 Loss: 0.027955422177910805\n",
            "Epoch 4 Batch 179/179 Loss: 0.017294852063059807\n",
            "Epoch 4/10 Avg Loss: 0.026145365021540135\n",
            "Epoch 5 Batch 1/179 Loss: 0.019215311855077744\n",
            "Epoch 5 Batch 2/179 Loss: 0.061830952763557434\n",
            "Epoch 5 Batch 3/179 Loss: 0.03220292180776596\n",
            "Epoch 5 Batch 4/179 Loss: 0.05582108348608017\n",
            "Epoch 5 Batch 5/179 Loss: 0.01153577584773302\n",
            "Epoch 5 Batch 6/179 Loss: 0.016376277431845665\n",
            "Epoch 5 Batch 7/179 Loss: 0.11007937043905258\n",
            "Epoch 5 Batch 8/179 Loss: 0.007859153673052788\n",
            "Epoch 5 Batch 9/179 Loss: 0.047859154641628265\n",
            "Epoch 5 Batch 10/179 Loss: 0.005185756832361221\n",
            "Epoch 5 Batch 11/179 Loss: 0.037637967616319656\n",
            "Epoch 5 Batch 12/179 Loss: 0.014976082369685173\n",
            "Epoch 5 Batch 13/179 Loss: 0.0041165221482515335\n",
            "Epoch 5 Batch 14/179 Loss: 0.048502322286367416\n",
            "Epoch 5 Batch 15/179 Loss: 0.09263631701469421\n",
            "Epoch 5 Batch 16/179 Loss: 0.05886174365878105\n",
            "Epoch 5 Batch 17/179 Loss: 0.02218317613005638\n",
            "Epoch 5 Batch 18/179 Loss: 0.0150202177464962\n",
            "Epoch 5 Batch 19/179 Loss: 0.06638690084218979\n",
            "Epoch 5 Batch 20/179 Loss: 0.0014899452216923237\n",
            "Epoch 5 Batch 21/179 Loss: 0.06409074366092682\n",
            "Epoch 5 Batch 22/179 Loss: 0.009074943140149117\n",
            "Epoch 5 Batch 23/179 Loss: 0.0010806964710354805\n",
            "Epoch 5 Batch 24/179 Loss: 0.011345981620252132\n",
            "Epoch 5 Batch 25/179 Loss: 0.0029794422443956137\n",
            "Epoch 5 Batch 26/179 Loss: 0.028687994927167892\n",
            "Epoch 5 Batch 27/179 Loss: 0.15466147661209106\n",
            "Epoch 5 Batch 28/179 Loss: 0.04607103765010834\n",
            "Epoch 5 Batch 29/179 Loss: 0.004894461948424578\n",
            "Epoch 5 Batch 30/179 Loss: 0.018844500184059143\n",
            "Epoch 5 Batch 31/179 Loss: 0.0027925940230488777\n",
            "Epoch 5 Batch 32/179 Loss: 0.01322874240577221\n",
            "Epoch 5 Batch 33/179 Loss: 0.1661817580461502\n",
            "Epoch 5 Batch 34/179 Loss: 0.006305623799562454\n",
            "Epoch 5 Batch 35/179 Loss: 0.0031854992266744375\n",
            "Epoch 5 Batch 36/179 Loss: 0.10529070347547531\n",
            "Epoch 5 Batch 37/179 Loss: 0.034338220953941345\n",
            "Epoch 5 Batch 38/179 Loss: 0.06091228127479553\n",
            "Epoch 5 Batch 39/179 Loss: 0.02092842012643814\n",
            "Epoch 5 Batch 40/179 Loss: 0.015642479062080383\n",
            "Epoch 5 Batch 41/179 Loss: 0.029156897217035294\n",
            "Epoch 5 Batch 42/179 Loss: 0.004265172407031059\n",
            "Epoch 5 Batch 43/179 Loss: 0.02213362418115139\n",
            "Epoch 5 Batch 44/179 Loss: 0.0177103653550148\n",
            "Epoch 5 Batch 45/179 Loss: 0.0405186265707016\n",
            "Epoch 5 Batch 46/179 Loss: 0.018555765971541405\n",
            "Epoch 5 Batch 47/179 Loss: 0.04942496865987778\n",
            "Epoch 5 Batch 48/179 Loss: 0.013892000541090965\n",
            "Epoch 5 Batch 49/179 Loss: 0.006393878720700741\n",
            "Epoch 5 Batch 50/179 Loss: 0.097887322306633\n",
            "Epoch 5 Batch 51/179 Loss: 0.00717923603951931\n",
            "Epoch 5 Batch 52/179 Loss: 0.008577408269047737\n",
            "Epoch 5 Batch 53/179 Loss: 0.008858703076839447\n",
            "Epoch 5 Batch 54/179 Loss: 0.05454842373728752\n",
            "Epoch 5 Batch 55/179 Loss: 0.028890561312437057\n",
            "Epoch 5 Batch 56/179 Loss: 0.17201165854930878\n",
            "Epoch 5 Batch 57/179 Loss: 0.019657302647829056\n",
            "Epoch 5 Batch 58/179 Loss: 0.010513968765735626\n",
            "Epoch 5 Batch 59/179 Loss: 0.0057281432673335075\n",
            "Epoch 5 Batch 60/179 Loss: 0.017230011522769928\n",
            "Epoch 5 Batch 61/179 Loss: 0.011187214404344559\n",
            "Epoch 5 Batch 62/179 Loss: 0.0031369172502309084\n",
            "Epoch 5 Batch 63/179 Loss: 0.04318198189139366\n",
            "Epoch 5 Batch 64/179 Loss: 0.015219542197883129\n",
            "Epoch 5 Batch 65/179 Loss: 0.034601181745529175\n",
            "Epoch 5 Batch 66/179 Loss: 0.03637199476361275\n",
            "Epoch 5 Batch 67/179 Loss: 0.01919868215918541\n",
            "Epoch 5 Batch 68/179 Loss: 0.033543191850185394\n",
            "Epoch 5 Batch 69/179 Loss: 0.003243726212531328\n",
            "Epoch 5 Batch 70/179 Loss: 0.03362732008099556\n",
            "Epoch 5 Batch 71/179 Loss: 0.02268875017762184\n",
            "Epoch 5 Batch 72/179 Loss: 0.05737990140914917\n",
            "Epoch 5 Batch 73/179 Loss: 0.04329157993197441\n",
            "Epoch 5 Batch 74/179 Loss: 0.22289308905601501\n",
            "Epoch 5 Batch 75/179 Loss: 0.03886091709136963\n",
            "Epoch 5 Batch 76/179 Loss: 0.006023642141371965\n",
            "Epoch 5 Batch 77/179 Loss: 0.008795535191893578\n",
            "Epoch 5 Batch 78/179 Loss: 0.025960493832826614\n",
            "Epoch 5 Batch 79/179 Loss: 0.10128293931484222\n",
            "Epoch 5 Batch 80/179 Loss: 0.02059875801205635\n",
            "Epoch 5 Batch 81/179 Loss: 0.021137194707989693\n",
            "Epoch 5 Batch 82/179 Loss: 0.028073415160179138\n",
            "Epoch 5 Batch 83/179 Loss: 0.025996319949626923\n",
            "Epoch 5 Batch 84/179 Loss: 0.007816494442522526\n",
            "Epoch 5 Batch 85/179 Loss: 0.03861022740602493\n",
            "Epoch 5 Batch 86/179 Loss: 0.007244599051773548\n",
            "Epoch 5 Batch 87/179 Loss: 0.02667805552482605\n",
            "Epoch 5 Batch 88/179 Loss: 0.008423659019172192\n",
            "Epoch 5 Batch 89/179 Loss: 0.1284143328666687\n",
            "Epoch 5 Batch 90/179 Loss: 0.012662959285080433\n",
            "Epoch 5 Batch 91/179 Loss: 0.01806352660059929\n",
            "Epoch 5 Batch 92/179 Loss: 0.035368870943784714\n",
            "Epoch 5 Batch 93/179 Loss: 0.022610794752836227\n",
            "Epoch 5 Batch 94/179 Loss: 0.0010387172224000096\n",
            "Epoch 5 Batch 95/179 Loss: 0.023028993979096413\n",
            "Epoch 5 Batch 96/179 Loss: 0.17377103865146637\n",
            "Epoch 5 Batch 97/179 Loss: 0.005496195517480373\n",
            "Epoch 5 Batch 98/179 Loss: 0.01065311674028635\n",
            "Epoch 5 Batch 99/179 Loss: 0.009235523641109467\n",
            "Epoch 5 Batch 100/179 Loss: 0.01032087579369545\n",
            "Epoch 5 Batch 101/179 Loss: 0.00615859217941761\n",
            "Epoch 5 Batch 102/179 Loss: 0.0121605908498168\n",
            "Epoch 5 Batch 103/179 Loss: 0.06332321465015411\n",
            "Epoch 5 Batch 104/179 Loss: 0.001560556935146451\n",
            "Epoch 5 Batch 105/179 Loss: 0.009179843589663506\n",
            "Epoch 5 Batch 106/179 Loss: 0.028403013944625854\n",
            "Epoch 5 Batch 107/179 Loss: 0.0544993057847023\n",
            "Epoch 5 Batch 108/179 Loss: 0.005495791789144278\n",
            "Epoch 5 Batch 109/179 Loss: 0.023491494357585907\n",
            "Epoch 5 Batch 110/179 Loss: 0.002077551558613777\n",
            "Epoch 5 Batch 111/179 Loss: 0.003027979750186205\n",
            "Epoch 5 Batch 112/179 Loss: 0.09996049106121063\n",
            "Epoch 5 Batch 113/179 Loss: 0.014562337659299374\n",
            "Epoch 5 Batch 114/179 Loss: 0.010775258764624596\n",
            "Epoch 5 Batch 115/179 Loss: 0.012400941923260689\n",
            "Epoch 5 Batch 116/179 Loss: 0.03276607021689415\n",
            "Epoch 5 Batch 117/179 Loss: 0.05990670248866081\n",
            "Epoch 5 Batch 118/179 Loss: 0.004468095023185015\n",
            "Epoch 5 Batch 119/179 Loss: 0.02339986525475979\n",
            "Epoch 5 Batch 120/179 Loss: 0.0551224909722805\n",
            "Epoch 5 Batch 121/179 Loss: 0.02414713427424431\n",
            "Epoch 5 Batch 122/179 Loss: 0.02326097898185253\n",
            "Epoch 5 Batch 123/179 Loss: 0.053087662905454636\n",
            "Epoch 5 Batch 124/179 Loss: 0.009217359125614166\n",
            "Epoch 5 Batch 125/179 Loss: 0.0029450301080942154\n",
            "Epoch 5 Batch 126/179 Loss: 0.039796262979507446\n",
            "Epoch 5 Batch 127/179 Loss: 0.0938415378332138\n",
            "Epoch 5 Batch 128/179 Loss: 0.014687316492199898\n",
            "Epoch 5 Batch 129/179 Loss: 0.05462104454636574\n",
            "Epoch 5 Batch 130/179 Loss: 0.003119327826425433\n",
            "Epoch 5 Batch 131/179 Loss: 0.013832435011863708\n",
            "Epoch 5 Batch 132/179 Loss: 0.03992508351802826\n",
            "Epoch 5 Batch 133/179 Loss: 0.06637796014547348\n",
            "Epoch 5 Batch 134/179 Loss: 0.025241365656256676\n",
            "Epoch 5 Batch 135/179 Loss: 0.04422962665557861\n",
            "Epoch 5 Batch 136/179 Loss: 0.01372903399169445\n",
            "Epoch 5 Batch 137/179 Loss: 0.03560342267155647\n",
            "Epoch 5 Batch 138/179 Loss: 0.05125919729471207\n",
            "Epoch 5 Batch 139/179 Loss: 0.009323926642537117\n",
            "Epoch 5 Batch 140/179 Loss: 0.024817489087581635\n",
            "Epoch 5 Batch 141/179 Loss: 0.017840184271335602\n",
            "Epoch 5 Batch 142/179 Loss: 0.009910151362419128\n",
            "Epoch 5 Batch 143/179 Loss: 0.07854355871677399\n",
            "Epoch 5 Batch 144/179 Loss: 0.021144086495041847\n",
            "Epoch 5 Batch 145/179 Loss: 0.007300994358956814\n",
            "Epoch 5 Batch 146/179 Loss: 0.027356304228305817\n",
            "Epoch 5 Batch 147/179 Loss: 0.07492528110742569\n",
            "Epoch 5 Batch 148/179 Loss: 0.12266672402620316\n",
            "Epoch 5 Batch 149/179 Loss: 0.005412905476987362\n",
            "Epoch 5 Batch 150/179 Loss: 0.006772313266992569\n",
            "Epoch 5 Batch 151/179 Loss: 0.05812961608171463\n",
            "Epoch 5 Batch 152/179 Loss: 0.012585222721099854\n",
            "Epoch 5 Batch 153/179 Loss: 0.025165481492877007\n",
            "Epoch 5 Batch 154/179 Loss: 0.1600685715675354\n",
            "Epoch 5 Batch 155/179 Loss: 0.0031478642486035824\n",
            "Epoch 5 Batch 156/179 Loss: 0.031471215188503265\n",
            "Epoch 5 Batch 157/179 Loss: 0.039989154785871506\n",
            "Epoch 5 Batch 158/179 Loss: 0.019664974883198738\n",
            "Epoch 5 Batch 159/179 Loss: 0.02752167545258999\n",
            "Epoch 5 Batch 160/179 Loss: 0.02205394208431244\n",
            "Epoch 5 Batch 161/179 Loss: 0.013953926041722298\n",
            "Epoch 5 Batch 162/179 Loss: 0.055619657039642334\n",
            "Epoch 5 Batch 163/179 Loss: 0.027930045500397682\n",
            "Epoch 5 Batch 164/179 Loss: 0.00842211488634348\n",
            "Epoch 5 Batch 165/179 Loss: 0.06994126737117767\n",
            "Epoch 5 Batch 166/179 Loss: 0.006237230263650417\n",
            "Epoch 5 Batch 167/179 Loss: 0.04849518835544586\n",
            "Epoch 5 Batch 168/179 Loss: 0.005509248003363609\n",
            "Epoch 5 Batch 169/179 Loss: 0.024131841957569122\n",
            "Epoch 5 Batch 170/179 Loss: 0.02113698422908783\n",
            "Epoch 5 Batch 171/179 Loss: 0.017895443364977837\n",
            "Epoch 5 Batch 172/179 Loss: 0.056205086410045624\n",
            "Epoch 5 Batch 173/179 Loss: 0.08885039389133453\n",
            "Epoch 5 Batch 174/179 Loss: 0.0714033991098404\n",
            "Epoch 5 Batch 175/179 Loss: 0.008386651054024696\n",
            "Epoch 5 Batch 176/179 Loss: 0.0011804935056716204\n",
            "Epoch 5 Batch 177/179 Loss: 0.007450615055859089\n",
            "Epoch 5 Batch 178/179 Loss: 0.013335781171917915\n",
            "Epoch 5 Batch 179/179 Loss: 0.02641676925122738\n",
            "Epoch 5/10 Avg Loss: 0.03373830506445413\n",
            "Epoch 6 Batch 1/179 Loss: 0.0035314231645315886\n",
            "Epoch 6 Batch 2/179 Loss: 0.03257237747311592\n",
            "Epoch 6 Batch 3/179 Loss: 0.027396447956562042\n",
            "Epoch 6 Batch 4/179 Loss: 0.023786339908838272\n",
            "Epoch 6 Batch 5/179 Loss: 0.0014645019546151161\n",
            "Epoch 6 Batch 6/179 Loss: 0.007024372927844524\n",
            "Epoch 6 Batch 7/179 Loss: 0.01860339753329754\n",
            "Epoch 6 Batch 8/179 Loss: 0.04422426223754883\n",
            "Epoch 6 Batch 9/179 Loss: 0.037004273384809494\n",
            "Epoch 6 Batch 10/179 Loss: 0.04716863855719566\n",
            "Epoch 6 Batch 11/179 Loss: 0.0030659143812954426\n",
            "Epoch 6 Batch 12/179 Loss: 0.03200489655137062\n",
            "Epoch 6 Batch 13/179 Loss: 0.0020737983286380768\n",
            "Epoch 6 Batch 14/179 Loss: 0.01203685812652111\n",
            "Epoch 6 Batch 15/179 Loss: 0.03141354024410248\n",
            "Epoch 6 Batch 16/179 Loss: 0.003118435386568308\n",
            "Epoch 6 Batch 17/179 Loss: 0.051804304122924805\n",
            "Epoch 6 Batch 18/179 Loss: 0.028398562222719193\n",
            "Epoch 6 Batch 19/179 Loss: 0.03707606717944145\n",
            "Epoch 6 Batch 20/179 Loss: 0.0071853334084153175\n",
            "Epoch 6 Batch 21/179 Loss: 0.04760348051786423\n",
            "Epoch 6 Batch 22/179 Loss: 0.0263536237180233\n",
            "Epoch 6 Batch 23/179 Loss: 0.003256811760365963\n",
            "Epoch 6 Batch 24/179 Loss: 0.02678859792649746\n",
            "Epoch 6 Batch 25/179 Loss: 0.04927859082818031\n",
            "Epoch 6 Batch 26/179 Loss: 0.10988512635231018\n",
            "Epoch 6 Batch 27/179 Loss: 0.011198196560144424\n",
            "Epoch 6 Batch 28/179 Loss: 0.011369769461452961\n",
            "Epoch 6 Batch 29/179 Loss: 0.010504316538572311\n",
            "Epoch 6 Batch 30/179 Loss: 0.00179158931132406\n",
            "Epoch 6 Batch 31/179 Loss: 0.004572325386106968\n",
            "Epoch 6 Batch 32/179 Loss: 0.01651868224143982\n",
            "Epoch 6 Batch 33/179 Loss: 0.012906667776405811\n",
            "Epoch 6 Batch 34/179 Loss: 0.013760763220489025\n",
            "Epoch 6 Batch 35/179 Loss: 0.0843224823474884\n",
            "Epoch 6 Batch 36/179 Loss: 0.006715714931488037\n",
            "Epoch 6 Batch 37/179 Loss: 0.005485464818775654\n",
            "Epoch 6 Batch 38/179 Loss: 0.002466951496899128\n",
            "Epoch 6 Batch 39/179 Loss: 0.07195045053958893\n",
            "Epoch 6 Batch 40/179 Loss: 0.03374604135751724\n",
            "Epoch 6 Batch 41/179 Loss: 0.08773548901081085\n",
            "Epoch 6 Batch 42/179 Loss: 0.06785993278026581\n",
            "Epoch 6 Batch 43/179 Loss: 0.002977867843583226\n",
            "Epoch 6 Batch 44/179 Loss: 0.0014403369277715683\n",
            "Epoch 6 Batch 45/179 Loss: 0.01386532187461853\n",
            "Epoch 6 Batch 46/179 Loss: 0.015308579429984093\n",
            "Epoch 6 Batch 47/179 Loss: 0.0038705947808921337\n",
            "Epoch 6 Batch 48/179 Loss: 0.08536276966333389\n",
            "Epoch 6 Batch 49/179 Loss: 0.018023058772087097\n",
            "Epoch 6 Batch 50/179 Loss: 0.01996106468141079\n",
            "Epoch 6 Batch 51/179 Loss: 0.009978690184652805\n",
            "Epoch 6 Batch 52/179 Loss: 0.0365014523267746\n",
            "Epoch 6 Batch 53/179 Loss: 0.008528260514140129\n",
            "Epoch 6 Batch 54/179 Loss: 0.01620863378047943\n",
            "Epoch 6 Batch 55/179 Loss: 0.017153097316622734\n",
            "Epoch 6 Batch 56/179 Loss: 0.004658862482756376\n",
            "Epoch 6 Batch 57/179 Loss: 0.0008527596946805716\n",
            "Epoch 6 Batch 58/179 Loss: 0.03228438273072243\n",
            "Epoch 6 Batch 59/179 Loss: 0.0017094158101826906\n",
            "Epoch 6 Batch 60/179 Loss: 0.021923493593931198\n",
            "Epoch 6 Batch 61/179 Loss: 0.0060670459643006325\n",
            "Epoch 6 Batch 62/179 Loss: 0.04142270237207413\n",
            "Epoch 6 Batch 63/179 Loss: 0.024085180833935738\n",
            "Epoch 6 Batch 64/179 Loss: 0.005674432963132858\n",
            "Epoch 6 Batch 65/179 Loss: 0.007695424370467663\n",
            "Epoch 6 Batch 66/179 Loss: 0.011309410445392132\n",
            "Epoch 6 Batch 67/179 Loss: 0.027903931215405464\n",
            "Epoch 6 Batch 68/179 Loss: 0.018811654299497604\n",
            "Epoch 6 Batch 69/179 Loss: 0.0244233887642622\n",
            "Epoch 6 Batch 70/179 Loss: 0.002118021948263049\n",
            "Epoch 6 Batch 71/179 Loss: 0.04424925148487091\n",
            "Epoch 6 Batch 72/179 Loss: 0.004439925774931908\n",
            "Epoch 6 Batch 73/179 Loss: 0.02017066441476345\n",
            "Epoch 6 Batch 74/179 Loss: 0.01295452006161213\n",
            "Epoch 6 Batch 75/179 Loss: 0.002724803052842617\n",
            "Epoch 6 Batch 76/179 Loss: 0.029382219538092613\n",
            "Epoch 6 Batch 77/179 Loss: 0.016406314447522163\n",
            "Epoch 6 Batch 78/179 Loss: 0.008848302997648716\n",
            "Epoch 6 Batch 79/179 Loss: 0.020965293049812317\n",
            "Epoch 6 Batch 80/179 Loss: 0.01962737925350666\n",
            "Epoch 6 Batch 81/179 Loss: 0.042348265647888184\n",
            "Epoch 6 Batch 82/179 Loss: 0.013402020558714867\n",
            "Epoch 6 Batch 83/179 Loss: 0.012637494131922722\n",
            "Epoch 6 Batch 84/179 Loss: 0.004103449173271656\n",
            "Epoch 6 Batch 85/179 Loss: 0.05917723476886749\n",
            "Epoch 6 Batch 86/179 Loss: 0.042812470346689224\n",
            "Epoch 6 Batch 87/179 Loss: 0.018384074792265892\n",
            "Epoch 6 Batch 88/179 Loss: 0.017980435863137245\n",
            "Epoch 6 Batch 89/179 Loss: 0.027665749192237854\n",
            "Epoch 6 Batch 90/179 Loss: 0.003927677869796753\n",
            "Epoch 6 Batch 91/179 Loss: 0.010501041077077389\n",
            "Epoch 6 Batch 92/179 Loss: 0.050262801349163055\n",
            "Epoch 6 Batch 93/179 Loss: 0.015021558851003647\n",
            "Epoch 6 Batch 94/179 Loss: 0.10232709348201752\n",
            "Epoch 6 Batch 95/179 Loss: 0.09884203970432281\n",
            "Epoch 6 Batch 96/179 Loss: 0.02195218950510025\n",
            "Epoch 6 Batch 97/179 Loss: 0.00462416373193264\n",
            "Epoch 6 Batch 98/179 Loss: 0.003226171713322401\n",
            "Epoch 6 Batch 99/179 Loss: 0.014556892216205597\n",
            "Epoch 6 Batch 100/179 Loss: 0.00609093252569437\n",
            "Epoch 6 Batch 101/179 Loss: 0.06161291524767876\n",
            "Epoch 6 Batch 102/179 Loss: 0.009705811738967896\n",
            "Epoch 6 Batch 103/179 Loss: 0.039571721106767654\n",
            "Epoch 6 Batch 104/179 Loss: 0.00424646120518446\n",
            "Epoch 6 Batch 105/179 Loss: 0.12873683869838715\n",
            "Epoch 6 Batch 106/179 Loss: 0.08895063400268555\n",
            "Epoch 6 Batch 107/179 Loss: 0.008985183201730251\n",
            "Epoch 6 Batch 108/179 Loss: 0.019395340234041214\n",
            "Epoch 6 Batch 109/179 Loss: 0.1546211987733841\n",
            "Epoch 6 Batch 110/179 Loss: 0.012168959714472294\n",
            "Epoch 6 Batch 111/179 Loss: 0.018404709175229073\n",
            "Epoch 6 Batch 112/179 Loss: 0.025934934616088867\n",
            "Epoch 6 Batch 113/179 Loss: 0.03916485235095024\n",
            "Epoch 6 Batch 114/179 Loss: 0.043868109583854675\n",
            "Epoch 6 Batch 115/179 Loss: 0.05417279526591301\n",
            "Epoch 6 Batch 116/179 Loss: 0.0029055264312773943\n",
            "Epoch 6 Batch 117/179 Loss: 0.010684625245630741\n",
            "Epoch 6 Batch 118/179 Loss: 0.13129541277885437\n",
            "Epoch 6 Batch 119/179 Loss: 0.05595206841826439\n",
            "Epoch 6 Batch 120/179 Loss: 0.04063382372260094\n",
            "Epoch 6 Batch 121/179 Loss: 0.036548614501953125\n",
            "Epoch 6 Batch 122/179 Loss: 0.12452956289052963\n",
            "Epoch 6 Batch 123/179 Loss: 0.005549305118620396\n",
            "Epoch 6 Batch 124/179 Loss: 0.003898191498592496\n",
            "Epoch 6 Batch 125/179 Loss: 0.02011188492178917\n",
            "Epoch 6 Batch 126/179 Loss: 0.01185671053826809\n",
            "Epoch 6 Batch 127/179 Loss: 0.042170457541942596\n",
            "Epoch 6 Batch 128/179 Loss: 0.007594857830554247\n",
            "Epoch 6 Batch 129/179 Loss: 0.004311081487685442\n",
            "Epoch 6 Batch 130/179 Loss: 0.009420689195394516\n",
            "Epoch 6 Batch 131/179 Loss: 0.004098182078450918\n",
            "Epoch 6 Batch 132/179 Loss: 0.042687952518463135\n",
            "Epoch 6 Batch 133/179 Loss: 0.043927691876888275\n",
            "Epoch 6 Batch 134/179 Loss: 0.022265471518039703\n",
            "Epoch 6 Batch 135/179 Loss: 0.05507298558950424\n",
            "Epoch 6 Batch 136/179 Loss: 0.026276614516973495\n",
            "Epoch 6 Batch 137/179 Loss: 0.01245246734470129\n",
            "Epoch 6 Batch 138/179 Loss: 0.20190630853176117\n",
            "Epoch 6 Batch 139/179 Loss: 0.006093874573707581\n",
            "Epoch 6 Batch 140/179 Loss: 0.01173173263669014\n",
            "Epoch 6 Batch 141/179 Loss: 0.06048169732093811\n",
            "Epoch 6 Batch 142/179 Loss: 0.04740621894598007\n",
            "Epoch 6 Batch 143/179 Loss: 0.0012836030218750238\n",
            "Epoch 6 Batch 144/179 Loss: 0.02051623910665512\n",
            "Epoch 6 Batch 145/179 Loss: 0.00397829245775938\n",
            "Epoch 6 Batch 146/179 Loss: 0.05424905940890312\n",
            "Epoch 6 Batch 147/179 Loss: 0.00477053364738822\n",
            "Epoch 6 Batch 148/179 Loss: 0.004522939212620258\n",
            "Epoch 6 Batch 149/179 Loss: 0.01706814020872116\n",
            "Epoch 6 Batch 150/179 Loss: 0.01028532162308693\n",
            "Epoch 6 Batch 151/179 Loss: 0.002752360887825489\n",
            "Epoch 6 Batch 152/179 Loss: 0.004226875491440296\n",
            "Epoch 6 Batch 153/179 Loss: 0.006279823370277882\n",
            "Epoch 6 Batch 154/179 Loss: 0.05812179297208786\n",
            "Epoch 6 Batch 155/179 Loss: 0.004387253429740667\n",
            "Epoch 6 Batch 156/179 Loss: 0.007435680367052555\n",
            "Epoch 6 Batch 157/179 Loss: 0.0056476471945643425\n",
            "Epoch 6 Batch 158/179 Loss: 0.14139556884765625\n",
            "Epoch 6 Batch 159/179 Loss: 0.02326672524213791\n",
            "Epoch 6 Batch 160/179 Loss: 0.027305856347084045\n",
            "Epoch 6 Batch 161/179 Loss: 0.010660404339432716\n",
            "Epoch 6 Batch 162/179 Loss: 0.01364184357225895\n",
            "Epoch 6 Batch 163/179 Loss: 0.015887217596173286\n",
            "Epoch 6 Batch 164/179 Loss: 0.005808470770716667\n",
            "Epoch 6 Batch 165/179 Loss: 0.17776232957839966\n",
            "Epoch 6 Batch 166/179 Loss: 0.024707920849323273\n",
            "Epoch 6 Batch 167/179 Loss: 0.04837241768836975\n",
            "Epoch 6 Batch 168/179 Loss: 0.00565345399081707\n",
            "Epoch 6 Batch 169/179 Loss: 0.04071393981575966\n",
            "Epoch 6 Batch 170/179 Loss: 0.01842106692492962\n",
            "Epoch 6 Batch 171/179 Loss: 0.07581730931997299\n",
            "Epoch 6 Batch 172/179 Loss: 0.00838708970695734\n",
            "Epoch 6 Batch 173/179 Loss: 0.05722954496741295\n",
            "Epoch 6 Batch 174/179 Loss: 0.011203078553080559\n",
            "Epoch 6 Batch 175/179 Loss: 0.024751784279942513\n",
            "Epoch 6 Batch 176/179 Loss: 0.0036063212901353836\n",
            "Epoch 6 Batch 177/179 Loss: 0.07242126017808914\n",
            "Epoch 6 Batch 178/179 Loss: 0.014950964599847794\n",
            "Epoch 6 Batch 179/179 Loss: 0.04796561226248741\n",
            "Epoch 6/10 Avg Loss: 0.029126353618459954\n",
            "Epoch 7 Batch 1/179 Loss: 0.0009177527390420437\n",
            "Epoch 7 Batch 2/179 Loss: 0.009515279904007912\n",
            "Epoch 7 Batch 3/179 Loss: 0.0016500341007485986\n",
            "Epoch 7 Batch 4/179 Loss: 0.011797207407653332\n",
            "Epoch 7 Batch 5/179 Loss: 0.00603660149499774\n",
            "Epoch 7 Batch 6/179 Loss: 0.22102782130241394\n",
            "Epoch 7 Batch 7/179 Loss: 0.028241626918315887\n",
            "Epoch 7 Batch 8/179 Loss: 0.02158735692501068\n",
            "Epoch 7 Batch 9/179 Loss: 0.029844768345355988\n",
            "Epoch 7 Batch 10/179 Loss: 0.025947265326976776\n",
            "Epoch 7 Batch 11/179 Loss: 0.027842646464705467\n",
            "Epoch 7 Batch 12/179 Loss: 0.10063604265451431\n",
            "Epoch 7 Batch 13/179 Loss: 0.018293235450983047\n",
            "Epoch 7 Batch 14/179 Loss: 0.07285641133785248\n",
            "Epoch 7 Batch 15/179 Loss: 0.015614875592291355\n",
            "Epoch 7 Batch 16/179 Loss: 0.043820228427648544\n",
            "Epoch 7 Batch 17/179 Loss: 0.019140023738145828\n",
            "Epoch 7 Batch 18/179 Loss: 0.01107708364725113\n",
            "Epoch 7 Batch 19/179 Loss: 0.0637442022562027\n",
            "Epoch 7 Batch 20/179 Loss: 0.024255767464637756\n",
            "Epoch 7 Batch 21/179 Loss: 0.08155204355716705\n",
            "Epoch 7 Batch 22/179 Loss: 0.04963432997465134\n",
            "Epoch 7 Batch 23/179 Loss: 0.007943621836602688\n",
            "Epoch 7 Batch 24/179 Loss: 0.006236115470528603\n",
            "Epoch 7 Batch 25/179 Loss: 0.01217589434236288\n",
            "Epoch 7 Batch 26/179 Loss: 0.042347557842731476\n",
            "Epoch 7 Batch 27/179 Loss: 0.010956279933452606\n",
            "Epoch 7 Batch 28/179 Loss: 0.006632647477090359\n",
            "Epoch 7 Batch 29/179 Loss: 0.003722948022186756\n",
            "Epoch 7 Batch 30/179 Loss: 0.02651798352599144\n",
            "Epoch 7 Batch 31/179 Loss: 0.014844019897282124\n",
            "Epoch 7 Batch 32/179 Loss: 0.01770077273249626\n",
            "Epoch 7 Batch 33/179 Loss: 0.019616877660155296\n",
            "Epoch 7 Batch 34/179 Loss: 0.07994772493839264\n",
            "Epoch 7 Batch 35/179 Loss: 0.017211511731147766\n",
            "Epoch 7 Batch 36/179 Loss: 0.007470644079148769\n",
            "Epoch 7 Batch 37/179 Loss: 0.004690500907599926\n",
            "Epoch 7 Batch 38/179 Loss: 0.010990310460329056\n",
            "Epoch 7 Batch 39/179 Loss: 0.007147400639951229\n",
            "Epoch 7 Batch 40/179 Loss: 0.016880352050065994\n",
            "Epoch 7 Batch 41/179 Loss: 0.006233463995158672\n",
            "Epoch 7 Batch 42/179 Loss: 0.005791302304714918\n",
            "Epoch 7 Batch 43/179 Loss: 0.007250811904668808\n",
            "Epoch 7 Batch 44/179 Loss: 0.009933398105204105\n",
            "Epoch 7 Batch 45/179 Loss: 0.09437862783670425\n",
            "Epoch 7 Batch 46/179 Loss: 0.009167840704321861\n",
            "Epoch 7 Batch 47/179 Loss: 0.05278586223721504\n",
            "Epoch 7 Batch 48/179 Loss: 0.016225522384047508\n",
            "Epoch 7 Batch 49/179 Loss: 0.0049694012850522995\n",
            "Epoch 7 Batch 50/179 Loss: 0.016315653920173645\n",
            "Epoch 7 Batch 51/179 Loss: 0.015156898647546768\n",
            "Epoch 7 Batch 52/179 Loss: 0.003740430111065507\n",
            "Epoch 7 Batch 53/179 Loss: 0.18810933828353882\n",
            "Epoch 7 Batch 54/179 Loss: 0.011526309885084629\n",
            "Epoch 7 Batch 55/179 Loss: 0.021040163934230804\n",
            "Epoch 7 Batch 56/179 Loss: 0.05771259963512421\n",
            "Epoch 7 Batch 57/179 Loss: 0.01740388199687004\n",
            "Epoch 7 Batch 58/179 Loss: 0.013040007092058659\n",
            "Epoch 7 Batch 59/179 Loss: 0.0040131923742592335\n",
            "Epoch 7 Batch 60/179 Loss: 0.012411623261868954\n",
            "Epoch 7 Batch 61/179 Loss: 0.03447824716567993\n",
            "Epoch 7 Batch 62/179 Loss: 0.01884838007390499\n",
            "Epoch 7 Batch 63/179 Loss: 0.006940382998436689\n",
            "Epoch 7 Batch 64/179 Loss: 0.05847674608230591\n",
            "Epoch 7 Batch 65/179 Loss: 0.11185207962989807\n",
            "Epoch 7 Batch 66/179 Loss: 0.009880729019641876\n",
            "Epoch 7 Batch 67/179 Loss: 0.0034988275729119778\n",
            "Epoch 7 Batch 68/179 Loss: 0.013339248485863209\n",
            "Epoch 7 Batch 69/179 Loss: 0.03909445181488991\n",
            "Epoch 7 Batch 70/179 Loss: 0.10058284550905228\n",
            "Epoch 7 Batch 71/179 Loss: 0.007697974797338247\n",
            "Epoch 7 Batch 72/179 Loss: 0.014888857491314411\n",
            "Epoch 7 Batch 73/179 Loss: 0.007595533970743418\n",
            "Epoch 7 Batch 74/179 Loss: 0.0062981825321912766\n",
            "Epoch 7 Batch 75/179 Loss: 0.05303683131933212\n",
            "Epoch 7 Batch 76/179 Loss: 0.009127220138907433\n",
            "Epoch 7 Batch 77/179 Loss: 0.003738915082067251\n",
            "Epoch 7 Batch 78/179 Loss: 0.020021816715598106\n",
            "Epoch 7 Batch 79/179 Loss: 0.0062749022617936134\n",
            "Epoch 7 Batch 80/179 Loss: 0.013357137329876423\n",
            "Epoch 7 Batch 81/179 Loss: 0.016313310712575912\n",
            "Epoch 7 Batch 82/179 Loss: 0.007116030436009169\n",
            "Epoch 7 Batch 83/179 Loss: 0.006369642913341522\n",
            "Epoch 7 Batch 84/179 Loss: 0.05217581242322922\n",
            "Epoch 7 Batch 85/179 Loss: 0.04384109377861023\n",
            "Epoch 7 Batch 86/179 Loss: 0.01014084741473198\n",
            "Epoch 7 Batch 87/179 Loss: 0.023366305977106094\n",
            "Epoch 7 Batch 88/179 Loss: 0.005474979989230633\n",
            "Epoch 7 Batch 89/179 Loss: 0.0013109829742461443\n",
            "Epoch 7 Batch 90/179 Loss: 0.020613864064216614\n",
            "Epoch 7 Batch 91/179 Loss: 0.03318265452980995\n",
            "Epoch 7 Batch 92/179 Loss: 0.03520568087697029\n",
            "Epoch 7 Batch 93/179 Loss: 0.0344221293926239\n",
            "Epoch 7 Batch 94/179 Loss: 0.04557758569717407\n",
            "Epoch 7 Batch 95/179 Loss: 0.061100125312805176\n",
            "Epoch 7 Batch 96/179 Loss: 0.006557739805430174\n",
            "Epoch 7 Batch 97/179 Loss: 0.12540288269519806\n",
            "Epoch 7 Batch 98/179 Loss: 0.010892839170992374\n",
            "Epoch 7 Batch 99/179 Loss: 0.025851484388113022\n",
            "Epoch 7 Batch 100/179 Loss: 0.026986826211214066\n",
            "Epoch 7 Batch 101/179 Loss: 0.047526806592941284\n",
            "Epoch 7 Batch 102/179 Loss: 0.022199753671884537\n",
            "Epoch 7 Batch 103/179 Loss: 0.009536877274513245\n",
            "Epoch 7 Batch 104/179 Loss: 0.035376593470573425\n",
            "Epoch 7 Batch 105/179 Loss: 0.025929218158125877\n",
            "Epoch 7 Batch 106/179 Loss: 0.02083531767129898\n",
            "Epoch 7 Batch 107/179 Loss: 0.020015647634863853\n",
            "Epoch 7 Batch 108/179 Loss: 0.10003720223903656\n",
            "Epoch 7 Batch 109/179 Loss: 0.07375034689903259\n",
            "Epoch 7 Batch 110/179 Loss: 0.057471927255392075\n",
            "Epoch 7 Batch 111/179 Loss: 0.032503314316272736\n",
            "Epoch 7 Batch 112/179 Loss: 0.012603153474628925\n",
            "Epoch 7 Batch 113/179 Loss: 0.013657666742801666\n",
            "Epoch 7 Batch 114/179 Loss: 0.003419639775529504\n",
            "Epoch 7 Batch 115/179 Loss: 0.00907508097589016\n",
            "Epoch 7 Batch 116/179 Loss: 0.06264511495828629\n",
            "Epoch 7 Batch 117/179 Loss: 0.002942949766293168\n",
            "Epoch 7 Batch 118/179 Loss: 0.01888078823685646\n",
            "Epoch 7 Batch 119/179 Loss: 0.0064572724513709545\n",
            "Epoch 7 Batch 120/179 Loss: 0.005253951530903578\n",
            "Epoch 7 Batch 121/179 Loss: 0.022892776876688004\n",
            "Epoch 7 Batch 122/179 Loss: 0.007665223442018032\n",
            "Epoch 7 Batch 123/179 Loss: 0.012127918191254139\n",
            "Epoch 7 Batch 124/179 Loss: 0.02889338880777359\n",
            "Epoch 7 Batch 125/179 Loss: 0.06461220234632492\n",
            "Epoch 7 Batch 126/179 Loss: 0.0341152623295784\n",
            "Epoch 7 Batch 127/179 Loss: 0.00842506904155016\n",
            "Epoch 7 Batch 128/179 Loss: 0.010973497293889523\n",
            "Epoch 7 Batch 129/179 Loss: 0.006472877226769924\n",
            "Epoch 7 Batch 130/179 Loss: 0.01606527715921402\n",
            "Epoch 7 Batch 131/179 Loss: 0.021533405408263206\n",
            "Epoch 7 Batch 132/179 Loss: 0.03294329345226288\n",
            "Epoch 7 Batch 133/179 Loss: 0.05166168883442879\n",
            "Epoch 7 Batch 134/179 Loss: 0.004409571178257465\n",
            "Epoch 7 Batch 135/179 Loss: 0.015312811359763145\n",
            "Epoch 7 Batch 136/179 Loss: 0.007855149917304516\n",
            "Epoch 7 Batch 137/179 Loss: 0.00455797603353858\n",
            "Epoch 7 Batch 138/179 Loss: 0.00566612184047699\n",
            "Epoch 7 Batch 139/179 Loss: 0.043492455035448074\n",
            "Epoch 7 Batch 140/179 Loss: 0.008770923130214214\n",
            "Epoch 7 Batch 141/179 Loss: 0.05968519300222397\n",
            "Epoch 7 Batch 142/179 Loss: 0.02921660803258419\n",
            "Epoch 7 Batch 143/179 Loss: 0.021889206022024155\n",
            "Epoch 7 Batch 144/179 Loss: 0.012211177498102188\n",
            "Epoch 7 Batch 145/179 Loss: 0.004403122700750828\n",
            "Epoch 7 Batch 146/179 Loss: 0.08216588944196701\n",
            "Epoch 7 Batch 147/179 Loss: 0.04851594567298889\n",
            "Epoch 7 Batch 148/179 Loss: 0.005950954742729664\n",
            "Epoch 7 Batch 149/179 Loss: 0.018398500978946686\n",
            "Epoch 7 Batch 150/179 Loss: 0.025699470192193985\n",
            "Epoch 7 Batch 151/179 Loss: 0.022193897515535355\n",
            "Epoch 7 Batch 152/179 Loss: 0.03335092216730118\n",
            "Epoch 7 Batch 153/179 Loss: 0.10482098162174225\n",
            "Epoch 7 Batch 154/179 Loss: 0.05339382216334343\n",
            "Epoch 7 Batch 155/179 Loss: 0.00987930130213499\n",
            "Epoch 7 Batch 156/179 Loss: 0.005909435451030731\n",
            "Epoch 7 Batch 157/179 Loss: 0.008393373340368271\n",
            "Epoch 7 Batch 158/179 Loss: 0.06084771454334259\n",
            "Epoch 7 Batch 159/179 Loss: 0.02514679916203022\n",
            "Epoch 7 Batch 160/179 Loss: 0.036344900727272034\n",
            "Epoch 7 Batch 161/179 Loss: 0.023706810548901558\n",
            "Epoch 7 Batch 162/179 Loss: 0.050701819360256195\n",
            "Epoch 7 Batch 163/179 Loss: 0.01874355971813202\n",
            "Epoch 7 Batch 164/179 Loss: 0.005825996398925781\n",
            "Epoch 7 Batch 165/179 Loss: 0.013912870548665524\n",
            "Epoch 7 Batch 166/179 Loss: 0.01003477443009615\n",
            "Epoch 7 Batch 167/179 Loss: 0.04673869162797928\n",
            "Epoch 7 Batch 168/179 Loss: 0.004758688621222973\n",
            "Epoch 7 Batch 169/179 Loss: 0.015005229040980339\n",
            "Epoch 7 Batch 170/179 Loss: 0.009699935093522072\n",
            "Epoch 7 Batch 171/179 Loss: 0.08003804832696915\n",
            "Epoch 7 Batch 172/179 Loss: 0.06285476684570312\n",
            "Epoch 7 Batch 173/179 Loss: 0.02648736909031868\n",
            "Epoch 7 Batch 174/179 Loss: 0.025234650820493698\n",
            "Epoch 7 Batch 175/179 Loss: 0.06863854080438614\n",
            "Epoch 7 Batch 176/179 Loss: 0.005202332511544228\n",
            "Epoch 7 Batch 177/179 Loss: 0.017504218965768814\n",
            "Epoch 7 Batch 178/179 Loss: 0.011690152809023857\n",
            "Epoch 7 Batch 179/179 Loss: 0.006626926362514496\n",
            "Epoch 7/10 Avg Loss: 0.028249119203563718\n",
            "Epoch 8 Batch 1/179 Loss: 0.012502247467637062\n",
            "Epoch 8 Batch 2/179 Loss: 0.0939885601401329\n",
            "Epoch 8 Batch 3/179 Loss: 0.02253885753452778\n",
            "Epoch 8 Batch 4/179 Loss: 0.05316820740699768\n",
            "Epoch 8 Batch 5/179 Loss: 0.0882696732878685\n",
            "Epoch 8 Batch 6/179 Loss: 0.015459736809134483\n",
            "Epoch 8 Batch 7/179 Loss: 0.0059934151358902454\n",
            "Epoch 8 Batch 8/179 Loss: 0.00888819806277752\n",
            "Epoch 8 Batch 9/179 Loss: 0.010956060141324997\n",
            "Epoch 8 Batch 10/179 Loss: 0.054022159427404404\n",
            "Epoch 8 Batch 11/179 Loss: 0.014289913699030876\n",
            "Epoch 8 Batch 12/179 Loss: 0.017470913007855415\n",
            "Epoch 8 Batch 13/179 Loss: 0.01761346124112606\n",
            "Epoch 8 Batch 14/179 Loss: 0.04532121866941452\n",
            "Epoch 8 Batch 15/179 Loss: 0.03327979892492294\n",
            "Epoch 8 Batch 16/179 Loss: 0.025481346994638443\n",
            "Epoch 8 Batch 17/179 Loss: 0.032619476318359375\n",
            "Epoch 8 Batch 18/179 Loss: 0.0344340056180954\n",
            "Epoch 8 Batch 19/179 Loss: 0.013256428763270378\n",
            "Epoch 8 Batch 20/179 Loss: 0.003054068423807621\n",
            "Epoch 8 Batch 21/179 Loss: 0.041640233248472214\n",
            "Epoch 8 Batch 22/179 Loss: 0.017506970092654228\n",
            "Epoch 8 Batch 23/179 Loss: 0.007303387857973576\n",
            "Epoch 8 Batch 24/179 Loss: 0.039851732552051544\n",
            "Epoch 8 Batch 25/179 Loss: 0.006755326874554157\n",
            "Epoch 8 Batch 26/179 Loss: 0.030251789838075638\n",
            "Epoch 8 Batch 27/179 Loss: 0.006201772019267082\n",
            "Epoch 8 Batch 28/179 Loss: 0.03675948083400726\n",
            "Epoch 8 Batch 29/179 Loss: 0.01098217535763979\n",
            "Epoch 8 Batch 30/179 Loss: 0.027604028582572937\n",
            "Epoch 8 Batch 31/179 Loss: 0.1044076681137085\n",
            "Epoch 8 Batch 32/179 Loss: 0.0076093245297670364\n",
            "Epoch 8 Batch 33/179 Loss: 0.0760798305273056\n",
            "Epoch 8 Batch 34/179 Loss: 0.10617148131132126\n",
            "Epoch 8 Batch 35/179 Loss: 0.00860882829874754\n",
            "Epoch 8 Batch 36/179 Loss: 0.019854865968227386\n",
            "Epoch 8 Batch 37/179 Loss: 0.027780037373304367\n",
            "Epoch 8 Batch 38/179 Loss: 0.14492711424827576\n",
            "Epoch 8 Batch 39/179 Loss: 0.03642365708947182\n",
            "Epoch 8 Batch 40/179 Loss: 0.015645377337932587\n",
            "Epoch 8 Batch 41/179 Loss: 0.014611007645726204\n",
            "Epoch 8 Batch 42/179 Loss: 0.0044752974063158035\n",
            "Epoch 8 Batch 43/179 Loss: 0.1206662654876709\n",
            "Epoch 8 Batch 44/179 Loss: 0.033119697123765945\n",
            "Epoch 8 Batch 45/179 Loss: 0.020902924239635468\n",
            "Epoch 8 Batch 46/179 Loss: 0.011893476359546185\n",
            "Epoch 8 Batch 47/179 Loss: 0.01841924712061882\n",
            "Epoch 8 Batch 48/179 Loss: 0.03138568624854088\n",
            "Epoch 8 Batch 49/179 Loss: 0.0056833834387362\n",
            "Epoch 8 Batch 50/179 Loss: 0.06649629026651382\n",
            "Epoch 8 Batch 51/179 Loss: 0.14349748194217682\n",
            "Epoch 8 Batch 52/179 Loss: 0.02732144109904766\n",
            "Epoch 8 Batch 53/179 Loss: 0.047238752245903015\n",
            "Epoch 8 Batch 54/179 Loss: 0.08671582490205765\n",
            "Epoch 8 Batch 55/179 Loss: 0.05535215139389038\n",
            "Epoch 8 Batch 56/179 Loss: 0.02638433314859867\n",
            "Epoch 8 Batch 57/179 Loss: 0.032014116644859314\n",
            "Epoch 8 Batch 58/179 Loss: 0.01498420536518097\n",
            "Epoch 8 Batch 59/179 Loss: 0.019690103828907013\n",
            "Epoch 8 Batch 60/179 Loss: 0.06419450789690018\n",
            "Epoch 8 Batch 61/179 Loss: 0.00978374108672142\n",
            "Epoch 8 Batch 62/179 Loss: 0.004098960198462009\n",
            "Epoch 8 Batch 63/179 Loss: 0.012027902528643608\n",
            "Epoch 8 Batch 64/179 Loss: 0.04329244792461395\n",
            "Epoch 8 Batch 65/179 Loss: 0.003841000609099865\n",
            "Epoch 8 Batch 66/179 Loss: 0.10146121680736542\n",
            "Epoch 8 Batch 67/179 Loss: 0.024196425452828407\n",
            "Epoch 8 Batch 68/179 Loss: 0.006971148774027824\n",
            "Epoch 8 Batch 69/179 Loss: 0.016139328479766846\n",
            "Epoch 8 Batch 70/179 Loss: 0.09159910678863525\n",
            "Epoch 8 Batch 71/179 Loss: 0.0814133957028389\n",
            "Epoch 8 Batch 72/179 Loss: 0.04341162368655205\n",
            "Epoch 8 Batch 73/179 Loss: 0.05551505088806152\n",
            "Epoch 8 Batch 74/179 Loss: 0.0018448296468704939\n",
            "Epoch 8 Batch 75/179 Loss: 0.08654604107141495\n",
            "Epoch 8 Batch 76/179 Loss: 0.06918337196111679\n",
            "Epoch 8 Batch 77/179 Loss: 0.015131251886487007\n",
            "Epoch 8 Batch 78/179 Loss: 0.045833077281713486\n",
            "Epoch 8 Batch 79/179 Loss: 0.005931992083787918\n",
            "Epoch 8 Batch 80/179 Loss: 0.2271902859210968\n",
            "Epoch 8 Batch 81/179 Loss: 0.012824514880776405\n",
            "Epoch 8 Batch 82/179 Loss: 0.07216514647006989\n",
            "Epoch 8 Batch 83/179 Loss: 0.02954401634633541\n",
            "Epoch 8 Batch 84/179 Loss: 0.07629501074552536\n",
            "Epoch 8 Batch 85/179 Loss: 0.06407065689563751\n",
            "Epoch 8 Batch 86/179 Loss: 0.0019757901318371296\n",
            "Epoch 8 Batch 87/179 Loss: 0.005976804997771978\n",
            "Epoch 8 Batch 88/179 Loss: 0.002089973771944642\n",
            "Epoch 8 Batch 89/179 Loss: 0.0391179583966732\n",
            "Epoch 8 Batch 90/179 Loss: 0.01366604957729578\n",
            "Epoch 8 Batch 91/179 Loss: 0.06924845278263092\n",
            "Epoch 8 Batch 92/179 Loss: 0.0021985871717333794\n",
            "Epoch 8 Batch 93/179 Loss: 0.00300011457875371\n",
            "Epoch 8 Batch 94/179 Loss: 0.07353386282920837\n",
            "Epoch 8 Batch 95/179 Loss: 0.0043970923870801926\n",
            "Epoch 8 Batch 96/179 Loss: 0.015160147100687027\n",
            "Epoch 8 Batch 97/179 Loss: 0.030010564252734184\n",
            "Epoch 8 Batch 98/179 Loss: 0.009039259515702724\n",
            "Epoch 8 Batch 99/179 Loss: 0.009994659572839737\n",
            "Epoch 8 Batch 100/179 Loss: 0.03619233891367912\n",
            "Epoch 8 Batch 101/179 Loss: 0.07798822224140167\n",
            "Epoch 8 Batch 102/179 Loss: 0.006433065980672836\n",
            "Epoch 8 Batch 103/179 Loss: 0.012814855203032494\n",
            "Epoch 8 Batch 104/179 Loss: 0.013077106326818466\n",
            "Epoch 8 Batch 105/179 Loss: 0.017471807077527046\n",
            "Epoch 8 Batch 106/179 Loss: 0.011626527644693851\n",
            "Epoch 8 Batch 107/179 Loss: 0.010386496782302856\n",
            "Epoch 8 Batch 108/179 Loss: 0.007646817248314619\n",
            "Epoch 8 Batch 109/179 Loss: 0.0032653147354722023\n",
            "Epoch 8 Batch 110/179 Loss: 0.023540008813142776\n",
            "Epoch 8 Batch 111/179 Loss: 0.06464844942092896\n",
            "Epoch 8 Batch 112/179 Loss: 0.00970575399696827\n",
            "Epoch 8 Batch 113/179 Loss: 0.05320288985967636\n",
            "Epoch 8 Batch 114/179 Loss: 0.044839851558208466\n",
            "Epoch 8 Batch 115/179 Loss: 0.006116152741014957\n",
            "Epoch 8 Batch 116/179 Loss: 0.004808590281754732\n",
            "Epoch 8 Batch 117/179 Loss: 0.0012998194433748722\n",
            "Epoch 8 Batch 118/179 Loss: 0.01066618598997593\n",
            "Epoch 8 Batch 119/179 Loss: 0.05208071693778038\n",
            "Epoch 8 Batch 120/179 Loss: 0.002703338162973523\n",
            "Epoch 8 Batch 121/179 Loss: 0.03141191229224205\n",
            "Epoch 8 Batch 122/179 Loss: 0.017155906185507774\n",
            "Epoch 8 Batch 123/179 Loss: 0.07509911060333252\n",
            "Epoch 8 Batch 124/179 Loss: 0.022866448387503624\n",
            "Epoch 8 Batch 125/179 Loss: 0.05690307170152664\n",
            "Epoch 8 Batch 126/179 Loss: 0.012024118565022945\n",
            "Epoch 8 Batch 127/179 Loss: 0.03447631001472473\n",
            "Epoch 8 Batch 128/179 Loss: 0.010965676978230476\n",
            "Epoch 8 Batch 129/179 Loss: 0.009536825120449066\n",
            "Epoch 8 Batch 130/179 Loss: 0.08602649718523026\n",
            "Epoch 8 Batch 131/179 Loss: 0.024966424331068993\n",
            "Epoch 8 Batch 132/179 Loss: 0.00482396874576807\n",
            "Epoch 8 Batch 133/179 Loss: 0.006344635039567947\n",
            "Epoch 8 Batch 134/179 Loss: 0.02574709802865982\n",
            "Epoch 8 Batch 135/179 Loss: 0.020677534863352776\n",
            "Epoch 8 Batch 136/179 Loss: 0.09040678292512894\n",
            "Epoch 8 Batch 137/179 Loss: 0.03185949847102165\n",
            "Epoch 8 Batch 138/179 Loss: 0.016602233052253723\n",
            "Epoch 8 Batch 139/179 Loss: 0.0043769292533397675\n",
            "Epoch 8 Batch 140/179 Loss: 0.019244937226176262\n",
            "Epoch 8 Batch 141/179 Loss: 0.005290035158395767\n",
            "Epoch 8 Batch 142/179 Loss: 0.01952248439192772\n",
            "Epoch 8 Batch 143/179 Loss: 0.04602198302745819\n",
            "Epoch 8 Batch 144/179 Loss: 0.000824703136458993\n",
            "Epoch 8 Batch 145/179 Loss: 0.029614249244332314\n",
            "Epoch 8 Batch 146/179 Loss: 0.030988719314336777\n",
            "Epoch 8 Batch 147/179 Loss: 0.032707784324884415\n",
            "Epoch 8 Batch 148/179 Loss: 0.023659445345401764\n",
            "Epoch 8 Batch 149/179 Loss: 0.059564780443906784\n",
            "Epoch 8 Batch 150/179 Loss: 0.005374358966946602\n",
            "Epoch 8 Batch 151/179 Loss: 0.10176832973957062\n",
            "Epoch 8 Batch 152/179 Loss: 0.0033973935060203075\n",
            "Epoch 8 Batch 153/179 Loss: 0.0010520955547690392\n",
            "Epoch 8 Batch 154/179 Loss: 0.006276986096054316\n",
            "Epoch 8 Batch 155/179 Loss: 0.004759471397846937\n",
            "Epoch 8 Batch 156/179 Loss: 0.006537334993481636\n",
            "Epoch 8 Batch 157/179 Loss: 0.0053414758294820786\n",
            "Epoch 8 Batch 158/179 Loss: 0.03479200229048729\n",
            "Epoch 8 Batch 159/179 Loss: 0.02501625195145607\n",
            "Epoch 8 Batch 160/179 Loss: 0.024413064122200012\n",
            "Epoch 8 Batch 161/179 Loss: 0.06121896952390671\n",
            "Epoch 8 Batch 162/179 Loss: 0.008319489657878876\n",
            "Epoch 8 Batch 163/179 Loss: 0.014780706726014614\n",
            "Epoch 8 Batch 164/179 Loss: 0.005625222809612751\n",
            "Epoch 8 Batch 165/179 Loss: 0.011945168487727642\n",
            "Epoch 8 Batch 166/179 Loss: 0.05680287629365921\n",
            "Epoch 8 Batch 167/179 Loss: 0.009657041169703007\n",
            "Epoch 8 Batch 168/179 Loss: 0.005256939679384232\n",
            "Epoch 8 Batch 169/179 Loss: 0.15995502471923828\n",
            "Epoch 8 Batch 170/179 Loss: 0.08205010741949081\n",
            "Epoch 8 Batch 171/179 Loss: 0.13617166876792908\n",
            "Epoch 8 Batch 172/179 Loss: 0.08582007139921188\n",
            "Epoch 8 Batch 173/179 Loss: 0.0036317685153335333\n",
            "Epoch 8 Batch 174/179 Loss: 0.03273750841617584\n",
            "Epoch 8 Batch 175/179 Loss: 0.01285010576248169\n",
            "Epoch 8 Batch 176/179 Loss: 0.09494125843048096\n",
            "Epoch 8 Batch 177/179 Loss: 0.009477461688220501\n",
            "Epoch 8 Batch 178/179 Loss: 0.0026098317466676235\n",
            "Epoch 8 Batch 179/179 Loss: 0.010898778215050697\n",
            "Epoch 8/10 Avg Loss: 0.03376225173582619\n",
            "Epoch 9 Batch 1/179 Loss: 0.04408799856901169\n",
            "Epoch 9 Batch 2/179 Loss: 0.05664226412773132\n",
            "Epoch 9 Batch 3/179 Loss: 0.08849000930786133\n",
            "Epoch 9 Batch 4/179 Loss: 0.018786264583468437\n",
            "Epoch 9 Batch 5/179 Loss: 0.11708580702543259\n",
            "Epoch 9 Batch 6/179 Loss: 0.07940824329853058\n",
            "Epoch 9 Batch 7/179 Loss: 0.02845056913793087\n",
            "Epoch 9 Batch 8/179 Loss: 0.02085772342979908\n",
            "Epoch 9 Batch 9/179 Loss: 0.008459488861262798\n",
            "Epoch 9 Batch 10/179 Loss: 0.014180903322994709\n",
            "Epoch 9 Batch 11/179 Loss: 0.02529807575047016\n",
            "Epoch 9 Batch 12/179 Loss: 0.11814949661493301\n",
            "Epoch 9 Batch 13/179 Loss: 0.014816878363490105\n",
            "Epoch 9 Batch 14/179 Loss: 0.15284815430641174\n",
            "Epoch 9 Batch 15/179 Loss: 0.01580549031496048\n",
            "Epoch 9 Batch 16/179 Loss: 0.0872630625963211\n",
            "Epoch 9 Batch 17/179 Loss: 0.012326334603130817\n",
            "Epoch 9 Batch 18/179 Loss: 0.022014278918504715\n",
            "Epoch 9 Batch 19/179 Loss: 0.008556472137570381\n",
            "Epoch 9 Batch 20/179 Loss: 0.10090862959623337\n",
            "Epoch 9 Batch 21/179 Loss: 0.1323530673980713\n",
            "Epoch 9 Batch 22/179 Loss: 0.013109852559864521\n",
            "Epoch 9 Batch 23/179 Loss: 0.004945701453834772\n",
            "Epoch 9 Batch 24/179 Loss: 0.02646922878921032\n",
            "Epoch 9 Batch 25/179 Loss: 0.057394176721572876\n",
            "Epoch 9 Batch 26/179 Loss: 0.005866491701453924\n",
            "Epoch 9 Batch 27/179 Loss: 0.003489585593342781\n",
            "Epoch 9 Batch 28/179 Loss: 0.0320674329996109\n",
            "Epoch 9 Batch 29/179 Loss: 0.05392516404390335\n",
            "Epoch 9 Batch 30/179 Loss: 0.019055737182497978\n",
            "Epoch 9 Batch 31/179 Loss: 0.024919405579566956\n",
            "Epoch 9 Batch 32/179 Loss: 0.015111811459064484\n",
            "Epoch 9 Batch 33/179 Loss: 0.004363317973911762\n",
            "Epoch 9 Batch 34/179 Loss: 0.007235174998641014\n",
            "Epoch 9 Batch 35/179 Loss: 0.021506862714886665\n",
            "Epoch 9 Batch 36/179 Loss: 0.06168699264526367\n",
            "Epoch 9 Batch 37/179 Loss: 0.008062818087637424\n",
            "Epoch 9 Batch 38/179 Loss: 0.07533155381679535\n",
            "Epoch 9 Batch 39/179 Loss: 0.05477820709347725\n",
            "Epoch 9 Batch 40/179 Loss: 0.02346133068203926\n",
            "Epoch 9 Batch 41/179 Loss: 0.16389530897140503\n",
            "Epoch 9 Batch 42/179 Loss: 0.0059521873481571674\n",
            "Epoch 9 Batch 43/179 Loss: 0.044076304882764816\n",
            "Epoch 9 Batch 44/179 Loss: 0.005212482530623674\n",
            "Epoch 9 Batch 45/179 Loss: 0.12130937725305557\n",
            "Epoch 9 Batch 46/179 Loss: 0.02026977390050888\n",
            "Epoch 9 Batch 47/179 Loss: 0.02584814466536045\n",
            "Epoch 9 Batch 48/179 Loss: 0.007870273664593697\n",
            "Epoch 9 Batch 49/179 Loss: 0.009074689820408821\n",
            "Epoch 9 Batch 50/179 Loss: 0.01191826444119215\n",
            "Epoch 9 Batch 51/179 Loss: 0.035114750266075134\n",
            "Epoch 9 Batch 52/179 Loss: 0.0129298847168684\n",
            "Epoch 9 Batch 53/179 Loss: 0.007116262800991535\n",
            "Epoch 9 Batch 54/179 Loss: 0.01645668037235737\n",
            "Epoch 9 Batch 55/179 Loss: 0.01597670651972294\n",
            "Epoch 9 Batch 56/179 Loss: 0.003014279529452324\n",
            "Epoch 9 Batch 57/179 Loss: 0.0018222398357465863\n",
            "Epoch 9 Batch 58/179 Loss: 0.007825261913239956\n",
            "Epoch 9 Batch 59/179 Loss: 0.034115925431251526\n",
            "Epoch 9 Batch 60/179 Loss: 0.004988791421055794\n",
            "Epoch 9 Batch 61/179 Loss: 0.028314705938100815\n",
            "Epoch 9 Batch 62/179 Loss: 0.12205272167921066\n",
            "Epoch 9 Batch 63/179 Loss: 0.023482341319322586\n",
            "Epoch 9 Batch 64/179 Loss: 0.06589753925800323\n",
            "Epoch 9 Batch 65/179 Loss: 0.1356944590806961\n",
            "Epoch 9 Batch 66/179 Loss: 0.02429785765707493\n",
            "Epoch 9 Batch 67/179 Loss: 0.007699300535023212\n",
            "Epoch 9 Batch 68/179 Loss: 0.002808777615427971\n",
            "Epoch 9 Batch 69/179 Loss: 0.020720116794109344\n",
            "Epoch 9 Batch 70/179 Loss: 0.06584794074296951\n",
            "Epoch 9 Batch 71/179 Loss: 0.11181563884019852\n",
            "Epoch 9 Batch 72/179 Loss: 0.026280272752046585\n",
            "Epoch 9 Batch 73/179 Loss: 0.006808235310018063\n",
            "Epoch 9 Batch 74/179 Loss: 0.07007730007171631\n",
            "Epoch 9 Batch 75/179 Loss: 0.004412331618368626\n",
            "Epoch 9 Batch 76/179 Loss: 0.011128144338726997\n",
            "Epoch 9 Batch 77/179 Loss: 0.004719246178865433\n",
            "Epoch 9 Batch 78/179 Loss: 0.03836067020893097\n",
            "Epoch 9 Batch 79/179 Loss: 0.07793060690164566\n",
            "Epoch 9 Batch 80/179 Loss: 0.027618641033768654\n",
            "Epoch 9 Batch 81/179 Loss: 0.017128394916653633\n",
            "Epoch 9 Batch 82/179 Loss: 0.021915758028626442\n",
            "Epoch 9 Batch 83/179 Loss: 0.005616021808236837\n",
            "Epoch 9 Batch 84/179 Loss: 0.015966439619660378\n",
            "Epoch 9 Batch 85/179 Loss: 0.01786176860332489\n",
            "Epoch 9 Batch 86/179 Loss: 0.04843540117144585\n",
            "Epoch 9 Batch 87/179 Loss: 0.05483810231089592\n",
            "Epoch 9 Batch 88/179 Loss: 0.16917948424816132\n",
            "Epoch 9 Batch 89/179 Loss: 0.06719598174095154\n",
            "Epoch 9 Batch 90/179 Loss: 0.07428023219108582\n",
            "Epoch 9 Batch 91/179 Loss: 0.02054590918123722\n",
            "Epoch 9 Batch 92/179 Loss: 0.02332630380988121\n",
            "Epoch 9 Batch 93/179 Loss: 0.006817235611379147\n",
            "Epoch 9 Batch 94/179 Loss: 0.0015504101756960154\n",
            "Epoch 9 Batch 95/179 Loss: 0.015536760911345482\n",
            "Epoch 9 Batch 96/179 Loss: 0.011342947371304035\n",
            "Epoch 9 Batch 97/179 Loss: 0.034078553318977356\n",
            "Epoch 9 Batch 98/179 Loss: 0.03060980699956417\n",
            "Epoch 9 Batch 99/179 Loss: 0.008110002614557743\n",
            "Epoch 9 Batch 100/179 Loss: 0.0055911969393491745\n",
            "Epoch 9 Batch 101/179 Loss: 0.0018349671736359596\n",
            "Epoch 9 Batch 102/179 Loss: 0.024743810296058655\n",
            "Epoch 9 Batch 103/179 Loss: 0.0031048846431076527\n",
            "Epoch 9 Batch 104/179 Loss: 0.007321457378566265\n",
            "Epoch 9 Batch 105/179 Loss: 0.03507189452648163\n",
            "Epoch 9 Batch 106/179 Loss: 0.09695153683423996\n",
            "Epoch 9 Batch 107/179 Loss: 0.021325349807739258\n",
            "Epoch 9 Batch 108/179 Loss: 0.027023961767554283\n",
            "Epoch 9 Batch 109/179 Loss: 0.062466055154800415\n",
            "Epoch 9 Batch 110/179 Loss: 0.015223448164761066\n",
            "Epoch 9 Batch 111/179 Loss: 0.06785315275192261\n",
            "Epoch 9 Batch 112/179 Loss: 0.027552418410778046\n",
            "Epoch 9 Batch 113/179 Loss: 0.011618658900260925\n",
            "Epoch 9 Batch 114/179 Loss: 0.03964758664369583\n",
            "Epoch 9 Batch 115/179 Loss: 0.008989227935671806\n",
            "Epoch 9 Batch 116/179 Loss: 0.019939862191677094\n",
            "Epoch 9 Batch 117/179 Loss: 0.10426189005374908\n",
            "Epoch 9 Batch 118/179 Loss: 0.008428355678915977\n",
            "Epoch 9 Batch 119/179 Loss: 0.0201064832508564\n",
            "Epoch 9 Batch 120/179 Loss: 0.013053574599325657\n",
            "Epoch 9 Batch 121/179 Loss: 0.016511885449290276\n",
            "Epoch 9 Batch 122/179 Loss: 0.01656249165534973\n",
            "Epoch 9 Batch 123/179 Loss: 0.021246571093797684\n",
            "Epoch 9 Batch 124/179 Loss: 0.022746456786990166\n",
            "Epoch 9 Batch 125/179 Loss: 0.01343123335391283\n",
            "Epoch 9 Batch 126/179 Loss: 0.009999409317970276\n",
            "Epoch 9 Batch 127/179 Loss: 0.12802092730998993\n",
            "Epoch 9 Batch 128/179 Loss: 0.03713156655430794\n",
            "Epoch 9 Batch 129/179 Loss: 0.006821609102189541\n",
            "Epoch 9 Batch 130/179 Loss: 0.0019435281865298748\n",
            "Epoch 9 Batch 131/179 Loss: 0.017190860584378242\n",
            "Epoch 9 Batch 132/179 Loss: 0.035930734127759933\n",
            "Epoch 9 Batch 133/179 Loss: 0.044714588671922684\n",
            "Epoch 9 Batch 134/179 Loss: 0.013230221346020699\n",
            "Epoch 9 Batch 135/179 Loss: 0.058593619614839554\n",
            "Epoch 9 Batch 136/179 Loss: 0.09322459995746613\n",
            "Epoch 9 Batch 137/179 Loss: 0.010490268468856812\n",
            "Epoch 9 Batch 138/179 Loss: 0.002875356934964657\n",
            "Epoch 9 Batch 139/179 Loss: 0.021144282072782516\n",
            "Epoch 9 Batch 140/179 Loss: 0.10248152166604996\n",
            "Epoch 9 Batch 141/179 Loss: 0.016761422157287598\n",
            "Epoch 9 Batch 142/179 Loss: 0.0034887979272753\n",
            "Epoch 9 Batch 143/179 Loss: 0.01436355710029602\n",
            "Epoch 9 Batch 144/179 Loss: 0.013755518943071365\n",
            "Epoch 9 Batch 145/179 Loss: 0.005189418792724609\n",
            "Epoch 9 Batch 146/179 Loss: 0.008903158828616142\n",
            "Epoch 9 Batch 147/179 Loss: 0.033039167523384094\n",
            "Epoch 9 Batch 148/179 Loss: 0.10285164415836334\n",
            "Epoch 9 Batch 149/179 Loss: 0.013241615146398544\n",
            "Epoch 9 Batch 150/179 Loss: 0.006773586384952068\n",
            "Epoch 9 Batch 151/179 Loss: 0.0008413302712142467\n",
            "Epoch 9 Batch 152/179 Loss: 0.01736942119896412\n",
            "Epoch 9 Batch 153/179 Loss: 0.02457406185567379\n",
            "Epoch 9 Batch 154/179 Loss: 0.0037619206123054028\n",
            "Epoch 9 Batch 155/179 Loss: 0.008027426898479462\n",
            "Epoch 9 Batch 156/179 Loss: 0.009486035443842411\n",
            "Epoch 9 Batch 157/179 Loss: 0.03513534367084503\n",
            "Epoch 9 Batch 158/179 Loss: 0.03288049250841141\n",
            "Epoch 9 Batch 159/179 Loss: 0.13597916066646576\n",
            "Epoch 9 Batch 160/179 Loss: 0.09757467359304428\n",
            "Epoch 9 Batch 161/179 Loss: 0.008185405284166336\n",
            "Epoch 9 Batch 162/179 Loss: 0.018605899065732956\n",
            "Epoch 9 Batch 163/179 Loss: 0.05933305621147156\n",
            "Epoch 9 Batch 164/179 Loss: 0.06367563456296921\n",
            "Epoch 9 Batch 165/179 Loss: 0.08253616094589233\n",
            "Epoch 9 Batch 166/179 Loss: 0.0444955937564373\n",
            "Epoch 9 Batch 167/179 Loss: 0.009693266823887825\n",
            "Epoch 9 Batch 168/179 Loss: 0.3075220584869385\n",
            "Epoch 9 Batch 169/179 Loss: 0.040150877088308334\n",
            "Epoch 9 Batch 170/179 Loss: 0.038478486239910126\n",
            "Epoch 9 Batch 171/179 Loss: 0.046450819820165634\n",
            "Epoch 9 Batch 172/179 Loss: 0.05206486955285072\n",
            "Epoch 9 Batch 173/179 Loss: 0.04788113385438919\n",
            "Epoch 9 Batch 174/179 Loss: 0.10512657463550568\n",
            "Epoch 9 Batch 175/179 Loss: 0.006847734097391367\n",
            "Epoch 9 Batch 176/179 Loss: 0.023655138909816742\n",
            "Epoch 9 Batch 177/179 Loss: 0.012495772913098335\n",
            "Epoch 9 Batch 178/179 Loss: 0.0420476570725441\n",
            "Epoch 9 Batch 179/179 Loss: 0.06433668732643127\n",
            "Epoch 9/10 Avg Loss: 0.037478447468384225\n",
            "Epoch 10 Batch 1/179 Loss: 0.006776113528758287\n",
            "Epoch 10 Batch 2/179 Loss: 0.01709042862057686\n",
            "Epoch 10 Batch 3/179 Loss: 0.011463919654488564\n",
            "Epoch 10 Batch 4/179 Loss: 0.0034768900368362665\n",
            "Epoch 10 Batch 5/179 Loss: 0.008238812908530235\n",
            "Epoch 10 Batch 6/179 Loss: 0.018818354234099388\n",
            "Epoch 10 Batch 7/179 Loss: 0.009258712641894817\n",
            "Epoch 10 Batch 8/179 Loss: 0.028362572193145752\n",
            "Epoch 10 Batch 9/179 Loss: 0.05548911914229393\n",
            "Epoch 10 Batch 10/179 Loss: 0.0032316576689481735\n",
            "Epoch 10 Batch 11/179 Loss: 0.007164228707551956\n",
            "Epoch 10 Batch 12/179 Loss: 0.00882519967854023\n",
            "Epoch 10 Batch 13/179 Loss: 0.0029986815061420202\n",
            "Epoch 10 Batch 14/179 Loss: 0.013243153691291809\n",
            "Epoch 10 Batch 15/179 Loss: 0.029902160167694092\n",
            "Epoch 10 Batch 16/179 Loss: 0.017309341579675674\n",
            "Epoch 10 Batch 17/179 Loss: 0.0035847178660333157\n",
            "Epoch 10 Batch 18/179 Loss: 0.010125309228897095\n",
            "Epoch 10 Batch 19/179 Loss: 0.0062910448759794235\n",
            "Epoch 10 Batch 20/179 Loss: 0.00876156147569418\n",
            "Epoch 10 Batch 21/179 Loss: 0.005294778849929571\n",
            "Epoch 10 Batch 22/179 Loss: 0.02036402001976967\n",
            "Epoch 10 Batch 23/179 Loss: 0.011276599019765854\n",
            "Epoch 10 Batch 24/179 Loss: 0.009647348895668983\n",
            "Epoch 10 Batch 25/179 Loss: 0.03681614622473717\n",
            "Epoch 10 Batch 26/179 Loss: 0.12874148786067963\n",
            "Epoch 10 Batch 27/179 Loss: 0.008259953930974007\n",
            "Epoch 10 Batch 28/179 Loss: 0.01204892061650753\n",
            "Epoch 10 Batch 29/179 Loss: 0.0012478752760216594\n",
            "Epoch 10 Batch 30/179 Loss: 0.005071217194199562\n",
            "Epoch 10 Batch 31/179 Loss: 0.009279532358050346\n",
            "Epoch 10 Batch 32/179 Loss: 0.00869643036276102\n",
            "Epoch 10 Batch 33/179 Loss: 0.06984513252973557\n",
            "Epoch 10 Batch 34/179 Loss: 0.13298793137073517\n",
            "Epoch 10 Batch 35/179 Loss: 0.006801888346672058\n",
            "Epoch 10 Batch 36/179 Loss: 0.0011903132544830441\n",
            "Epoch 10 Batch 37/179 Loss: 0.07397140562534332\n",
            "Epoch 10 Batch 38/179 Loss: 0.0007871061679907143\n",
            "Epoch 10 Batch 39/179 Loss: 0.0034195876214653254\n",
            "Epoch 10 Batch 40/179 Loss: 0.08467378467321396\n",
            "Epoch 10 Batch 41/179 Loss: 0.053113438189029694\n",
            "Epoch 10 Batch 42/179 Loss: 0.02838478982448578\n",
            "Epoch 10 Batch 43/179 Loss: 0.008326968178153038\n",
            "Epoch 10 Batch 44/179 Loss: 0.0898747369647026\n",
            "Epoch 10 Batch 45/179 Loss: 0.05680907890200615\n",
            "Epoch 10 Batch 46/179 Loss: 0.017545651644468307\n",
            "Epoch 10 Batch 47/179 Loss: 0.010497360490262508\n",
            "Epoch 10 Batch 48/179 Loss: 0.1396491676568985\n",
            "Epoch 10 Batch 49/179 Loss: 0.023846261203289032\n",
            "Epoch 10 Batch 50/179 Loss: 0.06583987921476364\n",
            "Epoch 10 Batch 51/179 Loss: 0.034788765013217926\n",
            "Epoch 10 Batch 52/179 Loss: 0.01042904518544674\n",
            "Epoch 10 Batch 53/179 Loss: 0.017405010759830475\n",
            "Epoch 10 Batch 54/179 Loss: 0.006906087510287762\n",
            "Epoch 10 Batch 55/179 Loss: 0.02960360422730446\n",
            "Epoch 10 Batch 56/179 Loss: 0.041769109666347504\n",
            "Epoch 10 Batch 57/179 Loss: 0.005466646980494261\n",
            "Epoch 10 Batch 58/179 Loss: 0.04595746472477913\n",
            "Epoch 10 Batch 59/179 Loss: 0.024474764242768288\n",
            "Epoch 10 Batch 60/179 Loss: 0.13044020533561707\n",
            "Epoch 10 Batch 61/179 Loss: 0.21251824498176575\n",
            "Epoch 10 Batch 62/179 Loss: 0.01655310019850731\n",
            "Epoch 10 Batch 63/179 Loss: 0.04626651853322983\n",
            "Epoch 10 Batch 64/179 Loss: 0.004784155171364546\n",
            "Epoch 10 Batch 65/179 Loss: 0.004861840046942234\n",
            "Epoch 10 Batch 66/179 Loss: 0.008218837901949883\n",
            "Epoch 10 Batch 67/179 Loss: 0.02022595703601837\n",
            "Epoch 10 Batch 68/179 Loss: 0.012005466036498547\n",
            "Epoch 10 Batch 69/179 Loss: 0.06431365013122559\n",
            "Epoch 10 Batch 70/179 Loss: 0.05530136078596115\n",
            "Epoch 10 Batch 71/179 Loss: 0.045121338218450546\n",
            "Epoch 10 Batch 72/179 Loss: 0.03520769253373146\n",
            "Epoch 10 Batch 73/179 Loss: 0.04814286530017853\n",
            "Epoch 10 Batch 74/179 Loss: 0.27727365493774414\n",
            "Epoch 10 Batch 75/179 Loss: 0.018838997930288315\n",
            "Epoch 10 Batch 76/179 Loss: 0.10548490285873413\n",
            "Epoch 10 Batch 77/179 Loss: 0.032182950526475906\n",
            "Epoch 10 Batch 78/179 Loss: 0.016008509323000908\n",
            "Epoch 10 Batch 79/179 Loss: 0.009230529889464378\n",
            "Epoch 10 Batch 80/179 Loss: 0.1586228609085083\n",
            "Epoch 10 Batch 81/179 Loss: 0.05914479121565819\n",
            "Epoch 10 Batch 82/179 Loss: 0.02587512880563736\n",
            "Epoch 10 Batch 83/179 Loss: 0.0437236987054348\n",
            "Epoch 10 Batch 84/179 Loss: 0.00711431261152029\n",
            "Epoch 10 Batch 85/179 Loss: 0.03163023665547371\n",
            "Epoch 10 Batch 86/179 Loss: 0.05374534800648689\n",
            "Epoch 10 Batch 87/179 Loss: 0.08118736743927002\n",
            "Epoch 10 Batch 88/179 Loss: 0.15073217451572418\n",
            "Epoch 10 Batch 89/179 Loss: 0.010936923325061798\n",
            "Epoch 10 Batch 90/179 Loss: 0.006589436903595924\n",
            "Epoch 10 Batch 91/179 Loss: 0.07117656618356705\n",
            "Epoch 10 Batch 92/179 Loss: 0.0746936947107315\n",
            "Epoch 10 Batch 93/179 Loss: 0.004497607704252005\n",
            "Epoch 10 Batch 94/179 Loss: 0.14940480887889862\n",
            "Epoch 10 Batch 95/179 Loss: 0.015831803902983665\n",
            "Epoch 10 Batch 96/179 Loss: 0.035906314849853516\n",
            "Epoch 10 Batch 97/179 Loss: 0.09315737336874008\n",
            "Epoch 10 Batch 98/179 Loss: 0.03785921260714531\n",
            "Epoch 10 Batch 99/179 Loss: 0.013597387820482254\n",
            "Epoch 10 Batch 100/179 Loss: 0.054822422564029694\n",
            "Epoch 10 Batch 101/179 Loss: 0.03600269928574562\n",
            "Epoch 10 Batch 102/179 Loss: 0.0029262222815304995\n",
            "Epoch 10 Batch 103/179 Loss: 0.016260851174592972\n",
            "Epoch 10 Batch 104/179 Loss: 0.06079348176717758\n",
            "Epoch 10 Batch 105/179 Loss: 0.011180433444678783\n",
            "Epoch 10 Batch 106/179 Loss: 0.019643716514110565\n",
            "Epoch 10 Batch 107/179 Loss: 0.02845942974090576\n",
            "Epoch 10 Batch 108/179 Loss: 0.0020381747744977474\n",
            "Epoch 10 Batch 109/179 Loss: 0.019652705639600754\n",
            "Epoch 10 Batch 110/179 Loss: 0.05153860151767731\n",
            "Epoch 10 Batch 111/179 Loss: 0.025470949709415436\n",
            "Epoch 10 Batch 112/179 Loss: 0.0162405576556921\n",
            "Epoch 10 Batch 113/179 Loss: 0.009076705202460289\n",
            "Epoch 10 Batch 114/179 Loss: 0.014376342296600342\n",
            "Epoch 10 Batch 115/179 Loss: 0.00550248846411705\n",
            "Epoch 10 Batch 116/179 Loss: 0.0180809386074543\n",
            "Epoch 10 Batch 117/179 Loss: 0.011331229470670223\n",
            "Epoch 10 Batch 118/179 Loss: 0.09612244367599487\n",
            "Epoch 10 Batch 119/179 Loss: 0.019765162840485573\n",
            "Epoch 10 Batch 120/179 Loss: 0.005052465945482254\n",
            "Epoch 10 Batch 121/179 Loss: 0.009254812262952328\n",
            "Epoch 10 Batch 122/179 Loss: 0.023729665204882622\n",
            "Epoch 10 Batch 123/179 Loss: 0.020951496437191963\n",
            "Epoch 10 Batch 124/179 Loss: 0.002778666326776147\n",
            "Epoch 10 Batch 125/179 Loss: 0.009952180087566376\n",
            "Epoch 10 Batch 126/179 Loss: 0.0084791025146842\n",
            "Epoch 10 Batch 127/179 Loss: 0.16218261420726776\n",
            "Epoch 10 Batch 128/179 Loss: 0.02273637056350708\n",
            "Epoch 10 Batch 129/179 Loss: 0.031001731753349304\n",
            "Epoch 10 Batch 130/179 Loss: 0.016175467520952225\n",
            "Epoch 10 Batch 131/179 Loss: 0.002503408119082451\n",
            "Epoch 10 Batch 132/179 Loss: 0.014834288507699966\n",
            "Epoch 10 Batch 133/179 Loss: 0.0017607497284188867\n",
            "Epoch 10 Batch 134/179 Loss: 0.028282754123210907\n",
            "Epoch 10 Batch 135/179 Loss: 0.013230584561824799\n",
            "Epoch 10 Batch 136/179 Loss: 0.20320677757263184\n",
            "Epoch 10 Batch 137/179 Loss: 0.05745796114206314\n",
            "Epoch 10 Batch 138/179 Loss: 0.031711455434560776\n",
            "Epoch 10 Batch 139/179 Loss: 0.021340476348996162\n",
            "Epoch 10 Batch 140/179 Loss: 0.008608642034232616\n",
            "Epoch 10 Batch 141/179 Loss: 0.026694636791944504\n",
            "Epoch 10 Batch 142/179 Loss: 0.0196298286318779\n",
            "Epoch 10 Batch 143/179 Loss: 0.0035379184409976006\n",
            "Epoch 10 Batch 144/179 Loss: 0.007758527062833309\n",
            "Epoch 10 Batch 145/179 Loss: 0.015036838129162788\n",
            "Epoch 10 Batch 146/179 Loss: 0.09057042747735977\n",
            "Epoch 10 Batch 147/179 Loss: 0.03972474858164787\n",
            "Epoch 10 Batch 148/179 Loss: 0.025365658104419708\n",
            "Epoch 10 Batch 149/179 Loss: 0.05312453955411911\n",
            "Epoch 10 Batch 150/179 Loss: 0.02358374372124672\n",
            "Epoch 10 Batch 151/179 Loss: 0.0018535447306931019\n",
            "Epoch 10 Batch 152/179 Loss: 0.013324310071766376\n",
            "Epoch 10 Batch 153/179 Loss: 0.04829246550798416\n",
            "Epoch 10 Batch 154/179 Loss: 0.0039575425907969475\n",
            "Epoch 10 Batch 155/179 Loss: 0.005160355009138584\n",
            "Epoch 10 Batch 156/179 Loss: 0.0019964794628322124\n",
            "Epoch 10 Batch 157/179 Loss: 0.004115702118724585\n",
            "Epoch 10 Batch 158/179 Loss: 0.006925823166966438\n",
            "Epoch 10 Batch 159/179 Loss: 0.011921041645109653\n",
            "Epoch 10 Batch 160/179 Loss: 0.015176751650869846\n",
            "Epoch 10 Batch 161/179 Loss: 0.0006084796041250229\n",
            "Epoch 10 Batch 162/179 Loss: 0.007833135314285755\n",
            "Epoch 10 Batch 163/179 Loss: 0.017964031547307968\n",
            "Epoch 10 Batch 164/179 Loss: 0.23628798127174377\n",
            "Epoch 10 Batch 165/179 Loss: 0.029669534415006638\n",
            "Epoch 10 Batch 166/179 Loss: 0.0027196689043194056\n",
            "Epoch 10 Batch 167/179 Loss: 0.028026342391967773\n",
            "Epoch 10 Batch 168/179 Loss: 0.02154206857085228\n",
            "Epoch 10 Batch 169/179 Loss: 0.011261135339736938\n",
            "Epoch 10 Batch 170/179 Loss: 0.011694401502609253\n",
            "Epoch 10 Batch 171/179 Loss: 0.008924886584281921\n",
            "Epoch 10 Batch 172/179 Loss: 0.027007445693016052\n",
            "Epoch 10 Batch 173/179 Loss: 0.026883268728852272\n",
            "Epoch 10 Batch 174/179 Loss: 0.024279793724417686\n",
            "Epoch 10 Batch 175/179 Loss: 0.01830168440937996\n",
            "Epoch 10 Batch 176/179 Loss: 0.02444082871079445\n",
            "Epoch 10 Batch 177/179 Loss: 0.007698133587837219\n",
            "Epoch 10 Batch 178/179 Loss: 0.005435348954051733\n",
            "Epoch 10 Batch 179/179 Loss: 0.11470329016447067\n",
            "Epoch 10/10 Avg Loss: 0.03439932215572517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testing_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, label in test_loader:\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(img)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(label.cpu().numpy())\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmRuRjGLlz6",
        "outputId": "ce8458bb-3ded-475c-da41-3997a6b872c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[295   3   0   2]\n",
            " [  2 304   0   0]\n",
            " [  1   0 404   0]\n",
            " [  3   2   0 295]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_without_last = torch.nn.Sequential(*list(model.seq[:-1]))\n",
        "img, label = testing_dataset[2]\n",
        "img = img.unsqueeze(0).to(device)\n",
        "\n",
        "encoder_without_last.eval()\n",
        "with torch.no_grad():\n",
        "    vec64 = encoder_without_last(img)\n",
        "\n",
        "print(vec64.shape)\n",
        "print(vec64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXMzt5cmN_L8",
        "outputId": "c0fa2651-d605-4d5c-9061-02c70214c9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64])\n",
            "tensor([[ 0.1973,  0.2766,  3.5412,  1.3700, -0.0384,  2.6749,  2.6655,  1.6314,\n",
            "          2.4932,  3.3673,  3.6743,  2.8183,  2.5718, -1.2120,  4.0023,  4.6217,\n",
            "          3.1204,  0.6196,  3.1137, -0.0112,  3.7586,  3.1182,  2.7705, -0.1004,\n",
            "         -0.0629,  4.0199,  3.2874,  4.1122, -0.0422,  0.6638, -0.0688,  3.6420,\n",
            "          1.6282,  0.6084,  0.0543,  1.0986,  0.0724,  1.1963, -0.3095,  1.2810,\n",
            "          2.9538,  2.9248, -0.0166,  0.5912,  1.6326,  0.6679, -0.0746,  3.0370,\n",
            "          1.6555,  1.6974,  4.4818,  2.9519, -0.2314,  1.1760,  3.1519, -1.6157,\n",
            "          2.8951, -0.1856,  2.6692, -0.0632,  1.2763,  1.6137, -0.1748,  3.4273]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model.eval()\n",
        "encoder = torch.nn.Sequential(*list(model.seq[:-1]))\n",
        "\n",
        "# deterministic transform for vector extraction\n",
        "base_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((300, 300)),\n",
        "    torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "def extract_vectors(dataset, output_csv):\n",
        "    rows = []\n",
        "\n",
        "    for i, (img, label) in enumerate(dataset):\n",
        "        filepath, _ = dataset.samples[i]\n",
        "        filename = os.path.basename(filepath)\n",
        "\n",
        "        img = img.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vec = encoder(img).squeeze(0).cpu().numpy()\n",
        "\n",
        "        rows.append([filename] + vec.tolist() + [label])\n",
        "\n",
        "    # write CSV\n",
        "    with open(output_csv, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        header = [\"filename\"] + [f\"v{i}\" for i in range(64)] + [\"label\"]\n",
        "        writer.writerow(header)\n",
        "        writer.writerows(rows)\n",
        "\n",
        "\n",
        "# training dataset\n",
        "training_dataset_wo_aug = torchvision.datasets.ImageFolder(\n",
        "    root=root_path + \"/Training\",\n",
        "    transform=base_transform\n",
        ")\n",
        "\n",
        "# testing dataset\n",
        "testing_dataset_wo_aug = torchvision.datasets.ImageFolder(\n",
        "    root=root_path + \"/Testing\",\n",
        "    transform=base_transform\n",
        ")\n",
        "\n",
        "# extract both\n",
        "extract_vectors(training_dataset_wo_aug, \"train_vectors.csv\")\n",
        "extract_vectors(testing_dataset_wo_aug, \"test_vectors.csv\")\n",
        "\n",
        "print(\"Done: train_vectors.csv and test_vectors.csv created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPlImICdRs_s",
        "outputId": "ace8bd5f-9991-4787-cbb1-8641b780f2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: train_vectors.csv and test_vectors.csv created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# prepare dataset\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(\n",
        "    root=root_path + \"/Training\",\n",
        "    transform=base_transform\n",
        ")\n",
        "\n",
        "model = EncoderModel().to(device)\n",
        "if os.path.exists(\"EncoderModel.pth\"):\n",
        "    model.load_state_dict(torch.load(\"EncoderModel.pth\"))\n",
        "\n",
        "# encoder (your feature extractor)\n",
        "model.eval()\n",
        "encoder = torch.nn.Sequential(*list(model.seq[:-1]))\n",
        "\n",
        "# collect 10 samples per class\n",
        "class_samples = {}\n",
        "for i, (img, label) in enumerate(dataset):\n",
        "    if label not in class_samples:\n",
        "        class_samples[label] = []\n",
        "    if len(class_samples[label]) < 10:\n",
        "        class_samples[label].append((img, label))\n",
        "\n",
        "    # stop if all 4 classes collected 10\n",
        "    if len(class_samples) == 4 and all(len(v) == 10 for v in class_samples.values()):\n",
        "        break\n",
        "\n",
        "# extract features\n",
        "vectors = []\n",
        "labels = []\n",
        "\n",
        "for label, samples in class_samples.items():\n",
        "    for img, _ in samples:\n",
        "        img = img.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            vec = encoder(img).squeeze(0).cpu().numpy()\n",
        "        vectors.append(vec)\n",
        "        labels.append(label)\n",
        "\n",
        "vectors = np.array(vectors)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# scale features\n",
        "X_scaled = StandardScaler().fit_transform(vectors)\n",
        "\n",
        "# PCA 3 components\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# plot 3D\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# FIX: manual colors per class (no colorbar)\n",
        "colors = ['red', 'blue', 'green', 'orange']\n",
        "point_colors = [colors[l] for l in labels]\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    X_pca[:, 0],\n",
        "    X_pca[:, 1],\n",
        "    X_pca[:, 2],\n",
        "    c=point_colors,\n",
        "    alpha=0.8,\n",
        "    s=60\n",
        ")\n",
        "\n",
        "# FIX: legend for each class\n",
        "for i, c in enumerate(colors):\n",
        "    ax.scatter([], [], [], color=c, label=f\"Class {i}\")\n",
        "\n",
        "ax.legend()\n",
        "\n",
        "# labels\n",
        "ax.set_xlabel(\"PC1\")\n",
        "ax.set_ylabel(\"PC2\")\n",
        "ax.set_zlabel(\"PC3\")\n",
        "plt.title(\"3D PCA of 10 Images per Class\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JKy6SyZsB_KF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}